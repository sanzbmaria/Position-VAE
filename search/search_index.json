{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Position_VAE","text":"<p>This Git repository contains code for exploring and preprocessing data, as well as implementing a Variational Autoencoder (VAE) model and visualizing the learned manifold of the model.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install, clone this repository:</p> <pre><code>git clone https://github.com/sanzbmaria/Position_VAE\n</code></pre> <p>Create and activate a new virtual environment named \"vae\" using conda, and install the required dependencies using the provided requirements file:</p> <pre><code>conda create --name vae &amp;&amp; conda activate vae &amp;&amp; conda config --append channels conda-forge &amp;&amp; conda install --file requirements.txt\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Before training the model, the original data should be stored in src/data/original/json, and processed using the following command:</p> <pre><code>python3 src/preprocessing/main.py --log_directory=src/preprocessing/logs --input_directory=src/data/original/ --output_directory=src/data/processed/\n</code></pre> <p>After processing the data the model can be run using : </p> <pre><code>python3 src/VAE/main.py -c src/VAE/congfigs/config.yml\n</code></pre> <p>To visualize the training progress, run the following command:</p> <pre><code>tensorboard --logdir src/VAE/logdir \n</code></pre> <p>That's it! You should now be able to explore the learned manifold of the VAE model using the visualizations.</p>"},{"location":"#code-documentation","title":"Code Documentation","text":""},{"location":"#model","title":"Model","text":""},{"location":"#preprocessing","title":"Preprocessing","text":""},{"location":"#exploration","title":"Exploration","text":""},{"location":"#utils","title":"Utils","text":""},{"location":"Model/data/","title":"Data","text":"<p>PyTorch Lightning data module for VAEs. </p> <p>This module provides train, validation and test dataloaders for the freely moving NHP dataset .</p> Example <p>vae_data_module = VAEDataset(data_path='./my_dataset.pt', train_batch_size=32, val_batch_size=64) trainer.fit(model, vae_data_module)</p>"},{"location":"Model/data/#src.VAE.data.NHPDataset","title":"<code>NHPDataset</code>","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>Time series dataset</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Time series data</p> required Source code in <code>src/VAE/data.py</code> <pre><code>class NHPDataset(torch.utils.data.Dataset):\n\"\"\"\n    Time series dataset\n\n    Args:\n        data (torch.Tensor): Time series data\n    \"\"\"\n\n    def __init__(self, data) -&gt; None:\n        super().__init__()\n        self.dataset = data\n\n    def __len__(self):\n        return self.dataset.__len__()\n\n    def __getitem__(self, index):\n\n        start = index * 1\n        end = start + 1\n\n        # block by block\n        return self.dataset[start:end]\n</code></pre>"},{"location":"Model/data/#src.VAE.data.VAEDataset","title":"<code>VAEDataset</code>","text":"<p>         Bases: <code>LightningDataModule</code></p> <p>PyTorch Lightning data module for VAEs</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the dataset</p> required <code>train_batch_size</code> <code>int</code> <p>Training batch size. Defaults to 8.</p> <code>8</code> <code>val_batch_size</code> <code>int</code> <p>Validation batch size. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers. Defaults to 24.</p> <code>24</code> <code>pin_memory</code> <code>bool</code> <p>Pin memory. Defaults to False.</p> <code>False</code> Source code in <code>src/VAE/data.py</code> <pre><code>class VAEDataset(LightningDataModule):\n\"\"\"\n    PyTorch Lightning data module for VAEs\n\n    Args:\n        data_path (str): Path to the dataset\n        train_batch_size (int, optional): Training batch size. Defaults to 8.\n        val_batch_size (int, optional): Validation batch size. Defaults to 8.\n        num_workers (int, optional): Number of workers. Defaults to 24.\n        pin_memory (bool, optional): Pin memory. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        train_batch_size: int = 8,\n        val_batch_size: int = 8,\n        num_workers: int = 24,\n        pin_memory: bool = False,\n        type: str = \"block\",\n        **kwargs,\n    ):\n        super().__init__()\n        self.data_dir = data_path\n        self.train_batch_size = train_batch_size\n        self.val_batch_size = val_batch_size\n        self.num_workers = num_workers\n        self.pin_memory = pin_memory\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n\"\"\"\n        Setup the dataset\n        Args:\n            stage (Optional[str], optional): Stage. Defaults to None.\n\n        \"\"\"\n        dataset = torch.load(self.data_dir)\n        # join the last  two dimensions\n        dataset = dataset.reshape(dataset.shape[0], -1)\n\n        # split the dataset\n        train_data, test_data = train_test_split(dataset, shuffle=False)\n        train_data, val_data = train_test_split(train_data, shuffle=False)\n\n        # scale the data\n\n        # scaler = MinMaxScaler(feature_range=(-1, 1))\n        # train_data = scaler.fit_transform(train_data)\n        # test_data = scaler.transform(test_data)\n        # val_data = scaler.transform(val_data)\n\n        # convert to tensor\n        # train_data = torch.from_numpy(train_data).float()\n        # test_data = torch.from_numpy(test_data).float()\n        # val_data = torch.from_numpy(val_data).float()\n\n        # create the dataset\n        self.train_dataset = NHPDataset(\n            train_data)\n        self.val_dataset = NHPDataset(\n            val_data)\n        self.test_dataset = NHPDataset(\n            test_data)\n\n    def train_dataloader(self):\n\"\"\"\n        Training dataloader\n        Returns:\n            DataLoader (torch.utils.data.DataLoader): Training dataloader\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.train_batch_size,\n            shuffle=True,\n            drop_last=True,\n            num_workers=self.num_workers,\n        )\n\n    def val_dataloader(self):\n\"\"\"\n        Validation dataloader\n        Returns:\n            DataLoader (torch.utils.data.DataLoader): Validation dataloader\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.val_batch_size,\n            shuffle=False,\n            drop_last=True,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n        )\n\n    def test_dataloader(self):\n\"\"\"\n        Test dataloader\n        Returns:\n            DataLoader (torch.utils.data.DataLoader): Test dataloader\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.val_batch_size,\n            shuffle=False,\n            drop_last=True,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"Model/data/#src.VAE.data.VAEDataset.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup the dataset</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>Stage. Defaults to None.</p> <code>None</code> Source code in <code>src/VAE/data.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n\"\"\"\n    Setup the dataset\n    Args:\n        stage (Optional[str], optional): Stage. Defaults to None.\n\n    \"\"\"\n    dataset = torch.load(self.data_dir)\n    # join the last  two dimensions\n    dataset = dataset.reshape(dataset.shape[0], -1)\n\n    # split the dataset\n    train_data, test_data = train_test_split(dataset, shuffle=False)\n    train_data, val_data = train_test_split(train_data, shuffle=False)\n\n    # scale the data\n\n    # scaler = MinMaxScaler(feature_range=(-1, 1))\n    # train_data = scaler.fit_transform(train_data)\n    # test_data = scaler.transform(test_data)\n    # val_data = scaler.transform(val_data)\n\n    # convert to tensor\n    # train_data = torch.from_numpy(train_data).float()\n    # test_data = torch.from_numpy(test_data).float()\n    # val_data = torch.from_numpy(val_data).float()\n\n    # create the dataset\n    self.train_dataset = NHPDataset(\n        train_data)\n    self.val_dataset = NHPDataset(\n        val_data)\n    self.test_dataset = NHPDataset(\n        test_data)\n</code></pre>"},{"location":"Model/data/#src.VAE.data.VAEDataset.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Test dataloader</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>torch.utils.data.DataLoader</code> <p>Test dataloader</p> Source code in <code>src/VAE/data.py</code> <pre><code>def test_dataloader(self):\n\"\"\"\n    Test dataloader\n    Returns:\n        DataLoader (torch.utils.data.DataLoader): Test dataloader\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.val_batch_size,\n        shuffle=False,\n        drop_last=True,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n    )\n</code></pre>"},{"location":"Model/data/#src.VAE.data.VAEDataset.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Training dataloader</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>torch.utils.data.DataLoader</code> <p>Training dataloader</p> Source code in <code>src/VAE/data.py</code> <pre><code>def train_dataloader(self):\n\"\"\"\n    Training dataloader\n    Returns:\n        DataLoader (torch.utils.data.DataLoader): Training dataloader\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.train_batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=self.num_workers,\n    )\n</code></pre>"},{"location":"Model/data/#src.VAE.data.VAEDataset.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Validation dataloader</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>torch.utils.data.DataLoader</code> <p>Validation dataloader</p> Source code in <code>src/VAE/data.py</code> <pre><code>def val_dataloader(self):\n\"\"\"\n    Validation dataloader\n    Returns:\n        DataLoader (torch.utils.data.DataLoader): Validation dataloader\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.val_batch_size,\n        shuffle=False,\n        drop_last=True,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n    )\n</code></pre>"},{"location":"Model/main/","title":"Main","text":"<p>This script defines the main function to train a Variational Autoencoder (VAE) model with PyTorch Lightning.</p> The main function does the following <ol> <li>Reads the configuration YAML file containing settings for the VAE model, data, and training parameters.</li> <li>Initializes TensorBoardLogger for logging and visualization.</li> <li>Initializes LearningRateMonitor for monitoring learning rates.</li> <li>Selects the appropriate plotting class based on the configuration.</li> <li>Defines the VAE model with the given settings.</li> <li>Defines the VAEDataset for loading and preprocessing data.</li> <li>Trains the VAE model using the Trainer class from PyTorch Lightning.</li> <li>Utilizes ModelCheckpoint for saving the best models.</li> <li>Utilizes EarlyStopping to stop training when the validation loss stops improving.</li> <li>Utilizes LearningRateFinder to find the optimal learning rate.</li> <li>Saves the configuration file to the log directory.</li> </ol> Example <p>python main.py --config_path configs/config.yml</p>"},{"location":"Model/model/","title":"Model (VAE)","text":"<p>This module defines a Variational Autoencoder (VAE) implemented using PyTorch and PyTorch Lightning.</p> Classes <ul> <li>Encoder: An encoder network used in the VAE.</li> <li>Decoder: A decoder network used in the VAE.</li> <li>VAE: The main class implementing the VAE.</li> </ul> Example <p>vae = VAE(in_dim=64,  hidden_dims=[32, 16, 8],  latent_dim=2,  learning_rate=1e-3,  batch_size=32,  num_workers=4,  gpus=1,  max_epochs=100,  early_stop_patience=10,  checkpoint_dir='checkpoints',  checkpoint_filename='vae.ckpt',  log_dir='logs',  log_name='vae',  plotter=Plot_NoLandmarks(),  **kwargs)</p>"},{"location":"Model/plotting/landmarks/","title":"Plotting Landmarks","text":"<p>Child Class that provides methods for visualizing the original and reconstructed data for the model for the LANDMARKS data.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the directory where the plots will be saved. Default is 'None'.</p> required <code>label_path</code> <code>str</code> <p>The path to the file containing the joint labels. Default is 'None'.</p> required <code>min_cluster_size</code> <code>int</code> <p>The minimum number of samples in a cluster. Default is 10.</p> required <code>umap_interval</code> <code>int</code> <p>The interval for applying UMAP to the data. Default is 5.</p> required <code>umap_input</code> <code>int</code> <p>The number of input samples for UMAP. Default is 1000.</p> required"},{"location":"Model/plotting/landmarks/#src.VAE.plots.plot156.Plot_Landmarks","title":"<code>Plot_Landmarks</code>","text":"<p>         Bases: <code>Plot</code></p> Source code in <code>src/VAE/plots/plot156.py</code> <pre><code>class Plot_Landmarks(Plot):\n    def __init__(self, data_path: str = 'None', label_path: str = 'None', min_cluster_size: int = 10, umap_interval : int = 5, umap_input: int = 1000, **kwargs):\n            super().__init__(data_path, label_path, min_cluster_size, umap_interval, umap_input)\n\n    def __call__(self, x: torch.tensor, xhat: torch.tensor, z:torch.tensor,  epoch: int, n : int = 10, **kwargs):\n        ###### NO LANDMARKS ######\n\n        assert x.shape[1] == 156, \"The input data should have 156 features\"\n        assert xhat.shape[1] == 156, \"The output data should have 156 features\"\n\n        x = x.reshape(-1, 156).to('cpu')\n        xhat = xhat.reshape(-1, 156).to('cpu')\n        z = z.squeeze(dim=1).to('cpu')\n\n        x_xyz_df, x_dist_hip_df, x_dist_lmks_df = self.divide_tensor(x)\n        xhat_xyz_df, xhat_dist_hip_df, xhat_dist_lmks_df = self.divide_tensor(xhat)\n\n        error_img = self.error_distribution_plot(x, xhat)\n        recreation_imgs = self.recreation_analysis(\n            x_xyz = x_xyz_df, \n            x_dist = x_dist_hip_df, \n            xhat_xyz = xhat_xyz_df,\n            xhat_dist = xhat_dist_hip_df, \n            x_lmk_dist = x_dist_lmks_df,   \n            xhat_lmk_dist = xhat_dist_lmks_df,  \n            z =z, \n            n_timepoints = n, \n            current_epoch = epoch)\n\n        # join the dicts\n        imgs_dict = {**error_img, **recreation_imgs}\n\n        return imgs_dict\n\n\n\n    def divide_tensor(self, tensor):\n\"\"\"\n        Divides a given tensor into separate dataframes for xyz values and distance from hip values for each joint.\n\n        Args:\n            tensor (torch.tensor): A tensor of shape(batch_size, 13, 12), where the 13x12 tensor has the following structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_13_x, joint_13_y, joint_13_z, joint_13_dist_hip]\n\n        Returns:\n            xyz_df(pandas dataframe): A dataframe with columns 'joint', 'x', 'y', and 'z', where each row corresponds to a joint and the xyz values for that joint.\n            dist_hip_df (pandas dataframe): A dataframe with columns 'joint' and 'dist', where each row corresponds to a joint and the distance from hip value for that joint.\n            df_landmarks (pandas dataframe): A dataframe with columns 'joint', 'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', 'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4', where each row corresponds to a joint\n        \"\"\"\n\n        tensor = tensor.reshape(-1, 13, 12)\n        # reshape the tensor into a 2D array with shape (batch * 13, 4)\n        reshaped_tensor = tensor.reshape(-1, 12)\n\n        df = pd.DataFrame(reshaped_tensor, \n                          columns=['x', 'y', 'z', \n                                   'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', \n                                   'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4',  \n                                   'dist_hip'])\n\n        # create a 'batch' column using np.indices and reshape to 1D array\n        batch_col = np.indices((tensor.shape[0], tensor.shape[1]))[0].reshape(-1)\n\n        df['joint'] = df.index % 13\n\n        # add the 'batch' column to the DataFrame\n        df['batch'] = batch_col\n\n        df.set_index('batch', inplace=True)\n\n        df.index.name = ''\n\n        # create a DataFrame with columns 'x', 'y', 'z', and 'joint'\n        df_xyz_joint = df.loc[:, ['x', 'y', 'z', 'joint']]\n\n        # create a DataFrame with columns 'joint' and 'dist_hip'\n        df_joint_dist_hip = df.loc[:, ['joint', 'dist_hip']]\n\n        df_landmarks = df.loc[:, ['joint', 'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', 'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4']]\n\n        return df_xyz_joint, df_joint_dist_hip, df_landmarks\n\n    def plot_landmark_distance_heatmap(self, diff):\n        # todo: plot the heatmap of the distance from landmarks\n        raise NotImplementedError\n</code></pre>"},{"location":"Model/plotting/landmarks/#src.VAE.plots.plot156.Plot_Landmarks.divide_tensor","title":"<code>divide_tensor(tensor)</code>","text":"<p>Divides a given tensor into separate dataframes for xyz values and distance from hip values for each joint.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>torch.tensor</code> <p>A tensor of shape(batch_size, 13, 12), where the 13x12 tensor has the following structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_13_x, joint_13_y, joint_13_z, joint_13_dist_hip]</p> required <p>Returns:</p> Name Type Description <code>xyz_df</code> <code>pandas dataframe</code> <p>A dataframe with columns 'joint', 'x', 'y', and 'z', where each row corresponds to a joint and the xyz values for that joint.</p> <code>dist_hip_df</code> <code>pandas dataframe</code> <p>A dataframe with columns 'joint' and 'dist', where each row corresponds to a joint and the distance from hip value for that joint.</p> <code>df_landmarks</code> <code>pandas dataframe</code> <p>A dataframe with columns 'joint', 'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', 'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4', where each row corresponds to a joint</p> Source code in <code>src/VAE/plots/plot156.py</code> <pre><code>def divide_tensor(self, tensor):\n\"\"\"\n    Divides a given tensor into separate dataframes for xyz values and distance from hip values for each joint.\n\n    Args:\n        tensor (torch.tensor): A tensor of shape(batch_size, 13, 12), where the 13x12 tensor has the following structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_13_x, joint_13_y, joint_13_z, joint_13_dist_hip]\n\n    Returns:\n        xyz_df(pandas dataframe): A dataframe with columns 'joint', 'x', 'y', and 'z', where each row corresponds to a joint and the xyz values for that joint.\n        dist_hip_df (pandas dataframe): A dataframe with columns 'joint' and 'dist', where each row corresponds to a joint and the distance from hip value for that joint.\n        df_landmarks (pandas dataframe): A dataframe with columns 'joint', 'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', 'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4', where each row corresponds to a joint\n    \"\"\"\n\n    tensor = tensor.reshape(-1, 13, 12)\n    # reshape the tensor into a 2D array with shape (batch * 13, 4)\n    reshaped_tensor = tensor.reshape(-1, 12)\n\n    df = pd.DataFrame(reshaped_tensor, \n                      columns=['x', 'y', 'z', \n                               'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', \n                               'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4',  \n                               'dist_hip'])\n\n    # create a 'batch' column using np.indices and reshape to 1D array\n    batch_col = np.indices((tensor.shape[0], tensor.shape[1]))[0].reshape(-1)\n\n    df['joint'] = df.index % 13\n\n    # add the 'batch' column to the DataFrame\n    df['batch'] = batch_col\n\n    df.set_index('batch', inplace=True)\n\n    df.index.name = ''\n\n    # create a DataFrame with columns 'x', 'y', 'z', and 'joint'\n    df_xyz_joint = df.loc[:, ['x', 'y', 'z', 'joint']]\n\n    # create a DataFrame with columns 'joint' and 'dist_hip'\n    df_joint_dist_hip = df.loc[:, ['joint', 'dist_hip']]\n\n    df_landmarks = df.loc[:, ['joint', 'dist_b1', 'dist_b2', 'dist_b3', 'dist_b4', 'dist_f1', 'dist_f2', 'dist_f3', 'dist_f4']]\n\n    return df_xyz_joint, df_joint_dist_hip, df_landmarks\n</code></pre>"},{"location":"Model/plotting/no_landmarks/","title":"NoLandmarks Plotting","text":"<p>Child Class that provides methods for visualizing the original and reconstructed data for the model for the NO LANDMARKS data.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the directory where the plots will be saved. Default is 'None'.</p> required <code>label_path</code> <code>str</code> <p>The path to the file containing the joint labels. Default is 'None'.</p> required <code>min_cluster_size</code> <code>int</code> <p>The minimum number of samples in a cluster. Default is 10.</p> required <code>umap_interval</code> <code>int</code> <p>The interval for applying UMAP to the data. Default is 5.</p> required <code>umap_input</code> <code>int</code> <p>The number of input samples for UMAP. Default is 1000.</p> required"},{"location":"Model/plotting/no_landmarks/#src.VAE.plots.plot52.Plot_NoLandmarks","title":"<code>Plot_NoLandmarks</code>","text":"<p>         Bases: <code>Plot</code></p> Source code in <code>src/VAE/plots/plot52.py</code> <pre><code>class Plot_NoLandmarks(Plot):\n\n    def __init__(self, data_path: str = 'None', label_path: str = 'None', min_cluster_size: int = 10, umap_interval : int = 5, umap_input: int = 1000, **kwargs):\n        super().__init__(data_path, label_path, min_cluster_size, umap_interval, umap_input)\n\n    def __call__(self, x: torch.Tensor, xhat: torch.Tensor, z:torch.Tensor,  epoch: int, n_timepoints : int = 10, **kwargs):\n\"\"\"\n        Computes error distribution and reconstruction analysis plots for the given input and output tensors.\n\n        Parameters:\n            x (torch.Tensor): A tensor of shape(batch_size, 52), where the 52 tensor has the following  structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]\n            xhat (torch.Tensor): A tensor of shape(batch_size, 52), where the 52 tensor has the following  structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]\n            z (torch.Tensor): A tensor of shape(batch_size, 1, latent_dim)\n            epoch (int): The current epoch\n            n_timepoints (int): The number of timepoints to plot\n\n        Returns:\n            imgs_dict (dict): A dictionary containing the images to be plotted\n\n        \"\"\"\n\n        ###### NO LANDMARKS ######\n\n        assert x.shape[1] == 52, \"The input data should have 52 features\"\n        assert xhat.shape[1] == 52, \"The output data should have 52 features\"\n\n        x = x.reshape(-1, 52).to('cpu')\n        xhat = xhat.reshape(-1, 52).to('cpu')\n        z = z.squeeze(dim=1).to('cpu')\n\n        x_xyz_df, x_dist_df = self.divide_tensor(x)\n        xhat_xyz_df, xhat_dist_df = self.divide_tensor(xhat)\n\n        error_img = self.error_distribution_plot(x, xhat)\n        recreation_imgs = self.recreation_analysis(x_xyz= x_xyz_df, x_dist= x_dist_df, xhat_xyz =xhat_xyz_df,xhat_dist= xhat_dist_df, z =z, n_timepoints = n_timepoints, current_epoch = epoch)\n\n        # join the dicts\n        imgs_dict = {**error_img, **recreation_imgs}\n\n        return imgs_dict\n\n    def divide_tensor(self, tensor: torch.Tensor):\n\"\"\"\n        Divides a given tensor into separate dataframes for xyz values and distance from hip values for each joint.\n\n        Args:\n            tensor (torch.Tensor): A tensor of shape(batch_size, 52), where the 52 tensor has the following structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]\n\n        Returns:\n            xyz_df (pandas dataframe): A dataframe with columns 'joint', 'x', 'y', and 'z', where each row corresponds to a joint and the xyz values for that joint.\n            dist_hip_df (pandas dataframe): A dataframe with columns 'joint' and 'dist', where each row corresponds to a joint and the distance from hip value for that joint.\n\n        \"\"\"\n        tensor = tensor.reshape(-1, 13, 4)\n\n        # reshape the tensor into a 2D array with shape (batch * 13, 4)\n        reshaped_tensor = tensor.reshape(-1, 4)\n\n        # create a DataFrame from the reshaped tensor\n        df = pd.DataFrame(reshaped_tensor, columns=['x', 'y', 'z', 'dist_hip'])\n\n        # create a 'batch' column using np.indices and reshape to 1D array\n        batch_col = np.indices((tensor.shape[0], tensor.shape[1]))[0].reshape(-1)\n\n        # add the 'batch' column to the DataFrame\n        df['batch'] = batch_col\n\n        df.set_index('batch', inplace=True)\n\n        df.index.name = ''\n        # display the resulting DataFrame\n\n        df['joint'] = list(range(13)) * (len(df) // 13)\n\n        # create a DataFrame with columns 'x', 'y', 'z', and 'joint'\n        df_xyz_joint = df.loc[:, ['x', 'y', 'z', 'joint']]\n\n        # create a DataFrame with columns 'joint' and 'dist_hip'\n        df_joint_dist_hip = df.loc[:, ['joint', 'dist_hip']]\n\n        return df_xyz_joint, df_joint_dist_hip\n</code></pre>"},{"location":"Model/plotting/no_landmarks/#src.VAE.plots.plot52.Plot_NoLandmarks.__call__","title":"<code>__call__(x, xhat, z, epoch, n_timepoints=10, **kwargs)</code>","text":"<p>Computes error distribution and reconstruction analysis plots for the given input and output tensors.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>A tensor of shape(batch_size, 52), where the 52 tensor has the following  structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]</p> required <code>xhat</code> <code>torch.Tensor</code> <p>A tensor of shape(batch_size, 52), where the 52 tensor has the following  structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]</p> required <code>z</code> <code>torch.Tensor</code> <p>A tensor of shape(batch_size, 1, latent_dim)</p> required <code>epoch</code> <code>int</code> <p>The current epoch</p> required <code>n_timepoints</code> <code>int</code> <p>The number of timepoints to plot</p> <code>10</code> <p>Returns:</p> Name Type Description <code>imgs_dict</code> <code>dict</code> <p>A dictionary containing the images to be plotted</p> Source code in <code>src/VAE/plots/plot52.py</code> <pre><code>def __call__(self, x: torch.Tensor, xhat: torch.Tensor, z:torch.Tensor,  epoch: int, n_timepoints : int = 10, **kwargs):\n\"\"\"\n    Computes error distribution and reconstruction analysis plots for the given input and output tensors.\n\n    Parameters:\n        x (torch.Tensor): A tensor of shape(batch_size, 52), where the 52 tensor has the following  structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]\n        xhat (torch.Tensor): A tensor of shape(batch_size, 52), where the 52 tensor has the following  structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]\n        z (torch.Tensor): A tensor of shape(batch_size, 1, latent_dim)\n        epoch (int): The current epoch\n        n_timepoints (int): The number of timepoints to plot\n\n    Returns:\n        imgs_dict (dict): A dictionary containing the images to be plotted\n\n    \"\"\"\n\n    ###### NO LANDMARKS ######\n\n    assert x.shape[1] == 52, \"The input data should have 52 features\"\n    assert xhat.shape[1] == 52, \"The output data should have 52 features\"\n\n    x = x.reshape(-1, 52).to('cpu')\n    xhat = xhat.reshape(-1, 52).to('cpu')\n    z = z.squeeze(dim=1).to('cpu')\n\n    x_xyz_df, x_dist_df = self.divide_tensor(x)\n    xhat_xyz_df, xhat_dist_df = self.divide_tensor(xhat)\n\n    error_img = self.error_distribution_plot(x, xhat)\n    recreation_imgs = self.recreation_analysis(x_xyz= x_xyz_df, x_dist= x_dist_df, xhat_xyz =xhat_xyz_df,xhat_dist= xhat_dist_df, z =z, n_timepoints = n_timepoints, current_epoch = epoch)\n\n    # join the dicts\n    imgs_dict = {**error_img, **recreation_imgs}\n\n    return imgs_dict\n</code></pre>"},{"location":"Model/plotting/no_landmarks/#src.VAE.plots.plot52.Plot_NoLandmarks.divide_tensor","title":"<code>divide_tensor(tensor)</code>","text":"<p>Divides a given tensor into separate dataframes for xyz values and distance from hip values for each joint.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>torch.Tensor</code> <p>A tensor of shape(batch_size, 52), where the 52 tensor has the following structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]</p> required <p>Returns:</p> Name Type Description <code>xyz_df</code> <code>pandas dataframe</code> <p>A dataframe with columns 'joint', 'x', 'y', and 'z', where each row corresponds to a joint and the xyz values for that joint.</p> <code>dist_hip_df</code> <code>pandas dataframe</code> <p>A dataframe with columns 'joint' and 'dist', where each row corresponds to a joint and the distance from hip value for that joint.</p> Source code in <code>src/VAE/plots/plot52.py</code> <pre><code>def divide_tensor(self, tensor: torch.Tensor):\n\"\"\"\n    Divides a given tensor into separate dataframes for xyz values and distance from hip values for each joint.\n\n    Args:\n        tensor (torch.Tensor): A tensor of shape(batch_size, 52), where the 52 tensor has the following structure: [joint_1_x, joint_1_y, joint_1_z, joint_1_dist_hip, ...,joint_n_x, joint_n_y, joint_n_z, joint_n_dist_hip]\n\n    Returns:\n        xyz_df (pandas dataframe): A dataframe with columns 'joint', 'x', 'y', and 'z', where each row corresponds to a joint and the xyz values for that joint.\n        dist_hip_df (pandas dataframe): A dataframe with columns 'joint' and 'dist', where each row corresponds to a joint and the distance from hip value for that joint.\n\n    \"\"\"\n    tensor = tensor.reshape(-1, 13, 4)\n\n    # reshape the tensor into a 2D array with shape (batch * 13, 4)\n    reshaped_tensor = tensor.reshape(-1, 4)\n\n    # create a DataFrame from the reshaped tensor\n    df = pd.DataFrame(reshaped_tensor, columns=['x', 'y', 'z', 'dist_hip'])\n\n    # create a 'batch' column using np.indices and reshape to 1D array\n    batch_col = np.indices((tensor.shape[0], tensor.shape[1]))[0].reshape(-1)\n\n    # add the 'batch' column to the DataFrame\n    df['batch'] = batch_col\n\n    df.set_index('batch', inplace=True)\n\n    df.index.name = ''\n    # display the resulting DataFrame\n\n    df['joint'] = list(range(13)) * (len(df) // 13)\n\n    # create a DataFrame with columns 'x', 'y', 'z', and 'joint'\n    df_xyz_joint = df.loc[:, ['x', 'y', 'z', 'joint']]\n\n    # create a DataFrame with columns 'joint' and 'dist_hip'\n    df_joint_dist_hip = df.loc[:, ['joint', 'dist_hip']]\n\n    return df_xyz_joint, df_joint_dist_hip\n</code></pre>"},{"location":"Model/plotting/plotting/","title":"Plotting","text":"<p>Parent Class that provides methods for visualizing the original and reconstructed data for the model.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the directory where the plots will be saved. Default is 'None'.</p> required <code>label_path</code> <code>str</code> <p>The path to the file containing the joint labels. Default is 'None'.</p> required <code>min_cluster_size</code> <code>int</code> <p>The minimum number of samples in a cluster. Default is 10.</p> required <code>umap_interval</code> <code>int</code> <p>The interval for applying UMAP to the data. Default is 5.</p> required <code>umap_input</code> <code>int</code> <p>The number of input samples for UMAP. Default is 1000.</p> required"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot","title":"<code>Plot</code>","text":"Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>class Plot:\n    def __init__(self, data_path: str = 'None', label_path: str = 'None', min_cluster_size: int = 10, umap_interval : int = 5, umap_input: int = 1000) -&gt; None:\n\"\"\"\n        Initializes the Plot class.\n\n        Args:\n            data_path (str): The path to the directory where the plots will be saved. Default is 'None'.\n            label_path (str): The path to the file containing the joint labels. Default is 'None'.\n            min_cluster_size (int): The minimum number of samples in a cluster. Default is 10.\n            umap_interval (int): The interval for applying UMAP to the data. Default is 5.\n            umap_input (int): The number of input samples for UMAP. Default is 1000.\n        \"\"\"\n\n        self._data_path = data_path\n        self._label_path = label_path\n        self.umap_interval = umap_interval \n        self.num_joints = 13\n        self.umap_input = umap_input\n\n        self.transform = transforms.Compose([transforms.ToTensor()])\n        self.hdbscan = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n\n        # check if the datapath exists if not create it\n        self._data_path = os.path.join(self._data_path, 'plots')\n\n        if not os.path.exists(self._data_path):\n            os.makedirs(self._data_path)\n\n\n        # load the labels\n        with open(self._label_path, 'rb') as f:\n            self.joint_labels = pickle.load(f)\n\n        if self.num_joints == 13:\n            self.joint_colors = {i: color for i, color in enumerate([\"lightcoral\", \"coral\", \"sienna\", \"plum\", \"tomato\", \"darkslateblue\", \"salmon\", \"seagreen\", \"mediumspringgreen\", \"springgreen\", \"mediumvioletred\", \"deeppink\", \"greenyellow\"])}\n            self.joint_graph = [[\"neck\",\"head\"],[\"neck\",\"RShoulder\"],[\"neck\",\"Lshoulder\"],[\"neck\",\"hip\"],[\"head\",\"nose\"],[\"RShoulder\",\"RHand\"],[\"Lshoulder\",\"Lhand\"],[\"hip\",\"RKnee\"],[\"hip\",\"LKnee\"],[\"hip\",\"tail\"],[\"LKnee\",\"Lfoot\"],[\"RKnee\",\"RFoot\"]]\n        else:\n            raise ValueError(\"Number of joints not supported\")\n\n        self.umap_reducer = UMAP(\n            n_neighbors=15,  # default 15, The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n            n_components=3,  # default 2, The dimension of the space to embed into.\n            metric=\"euclidean\",  # default 'euclidean', The metric to use to compute distances in high dimensional space.\n            n_epochs=3,  # default None, The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings.\n            learning_rate=1.0,  # default 1.0, The initial learning rate for the embedding optimization.\n            init=\"spectral\",  # default 'spectral', How to initialize the low dimensional embedding. Options are: {'spectral', 'random', A numpy array of initial embedding positions}.\n            min_dist=0.1,  # default 0.1, The effective minimum distance between embedded points.\n            spread=1.0,  # default 1.0, The effective scale of embedded points. In combination with ``min_dist`` this determines how clustered/clumped the embedded points are.\n        )\n\n\n        self.hdb = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n            gen_min_span_tree=False, leaf_size=100,\n            metric='euclidean', min_samples=None, p=None, min_cluster_size=10)\n\n    @abstractmethod\n    def __call__(self, **kwargs) -&gt; None:\n\"\"\"\n        Abstract method for plotting.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def divide_tensor(self, **kwargs):\n\"\"\"\n        Divides the tensor into the different parts (xyz, distance, landmark distance, etc)\n        \"\"\"\n        raise NotImplementedError\n\n    def plot_landmark_distance_heatmap(self, x_lmk_dist, xhat_lmk_dist):\n\"\"\"\n        Plots the heatmap of the landmark distance. \n        \"\"\"\n        raise NotImplementedError\n\n\n    def recreation_analysis(self, x_xyz : pd.DataFrame,\n                            xhat_xyz: pd.DataFrame,  \n                            x_dist: pd.DataFrame, \n                            xhat_dist: pd.DataFrame,\n                            x_lmk_dist=None,\n                            xhat_lmk_dist=None, \n                            z=None,\n                            n_timepoints=2, \n                            current_epoch=0):\n\"\"\"\n        Compares the original data with the reconstructed data\n\n        Args:\n            x_xyz (Pandas.df): original data xyz values\n            xhat_xyz (Pandas.df): reconstructed data xyz values\n            x_dist (Pandas.df):  original data distance from hip values  \n            xhat_dist (Pandas.df): reconstructed data distance from hip values\n            x_lmk_dist (Pandas.df): (optional: only for Landmark Version) original data distance from landmarks values\n            xhat_lmk_dist (Pandas.df): (optional: only for Landmark Version) reconstructed data distance from landmarks values\n            z (torch.tensor): (optional: only for Landmark Version) tensor of the model's encoded latent variables \n            n_timepoints (int): number of random samples to compare (pose only)\n            current_epoch (int): current epoch\n\n        Returns: \n            fig_dic (dict): dictionary with tensors of the figures\n        \"\"\"\n\n        # choose n random indexes to plot \n        if n_timepoints &gt; len(x_xyz.index.unique()):\n            n_timepoints = len(x_xyz.index.unique())\n\n        indexes = np.random.choice(len(x_xyz.index.unique()), n_timepoints, replace=False)\n\n        # plot the heatmaps\n\n        xyz_diff = self.calculate_differences(x_xyz, xhat_xyz)\n        hip_dist_diff = self.calculate_differences(x_dist, xhat_dist)\n\n        heatmaps_xyz_fig = self.plot_xyz_heatmap(xyz_diff)\n        heatmap_dist_fig = self.plot_hip_distance_heatmap(hip_dist_diff)\n\n        heatmap_dist_lmk_fig = None\n\n        if x_lmk_dist is not None:\n            # todo ! \n            # lmk_dist_diff = self.calculate_differences(x_lmk_dist, xhat_lmk_dist)\n            # heatmap_dist_lmk_fig = self.plot_landmark_distance_heatmap(lmk_dist_diff)\n            heatmap_dist_lmk_fig = None\n\n        # close the figures\n        matplotlib.pyplot.close()\n\n        # plot the joints in 3D space \n        joints_fig = self.plot_joints(original_df = x_xyz,recreated_df= xhat_xyz, indexes= indexes)\n\n        # close the figures\n        matplotlib.pyplot.close()\n\n        if z is not None and (current_epoch % self.umap_interval  == 1 or current_epoch == 1):\n            # plot the clusters using umap\n            cluster_fig = self.cluster_analysis(z)\n\n            # close the figures\n            matplotlib.pyplot.close()\n\n            # join the dictionarys into one\n            fig_dict = {**heatmaps_xyz_fig, **heatmap_dist_fig, **heatmap_dist_lmk_fig, **joints_fig, **cluster_fig} if heatmap_dist_lmk_fig is not None else {**heatmaps_xyz_fig, **heatmap_dist_fig, **joints_fig, **cluster_fig}\n        else:\n            fig_dict = {**heatmaps_xyz_fig, **heatmap_dist_fig, **heatmap_dist_lmk_fig, **joints_fig} if heatmap_dist_lmk_fig is not None else {**heatmaps_xyz_fig, **heatmap_dist_fig, **joints_fig}\n\n        tensor_dict = {}\n\n        for fig in fig_dict:\n            # open the picture and convert it to tensor\n\n            img = Image.open(fig_dict[fig])  \n            tensor_img = self.transform(img)\n            tensor_dict[fig] = tensor_img\n\n        return tensor_dict\n\n\n    def error_distribution_plot(self, x : torch.tensor , xhat : torch.tensor):\n\"\"\"\n        Plots a histogram of the error between the original and recreated data.\n\n        Args:\n            x (torch.Tensor): A tensor of shape (num_samples, 52/156) representing the original data.\n            xhat (torch.Tensor): A tensor of shape (num_samples, 52/156) representing the recreated data.\n\n        Returns:\n            dict (dic): A dictionary containing the name of the figure and the path to the figure.\n        \"\"\"\n\n        x = torch.reshape(x, (x.shape[0],  self.num_joints, -1))\n        xhat = torch.reshape(xhat, (xhat.shape[0],  self.num_joints, -1))\n\n\n        # Calculate the mean squared error (MSE) or mean absolute error (MAE) between the original and recreated data\n        errors = torch.mean((x - xhat) ** 2, dim=(1, 2))  # for MSE\n        # errors = torch.mean(torch.abs(original_data - recreated_data), dim=(1, 2))  # for MAE\n\n        fig, ax = plt.subplots()\n        ax.hist(errors.cpu().numpy(), bins=30, density=True, alpha=0.75)\n        ax.set_xlabel('Error Value')\n        ax.set_ylabel('Density')\n        ax.set_title('Error Distribution between Original and Recreated Data')\n\n        # save the figure\n        path = os.path.join(self._data_path, 'error_distribution_fig.png')\n        fig.savefig(path)\n\n        img = Image.open(path)  \n        tensor_img = self.transform(img)\n\n        return {'error_distribution_fig': tensor_img}\n\n\n    def cluster_plot(self, X: np.array, y :np.array = None):\n\"\"\"\n        Plots a 3D scatter plot of the input data X with consistent colors for the same label.\n\n        Args:\n            X (numpy.array): ndarray The input data to plot in 3D. It must have at least three columns.\n            y (numpy.array): ndarray The labels for the input data. If None, the labels will be generated automatically. (optional)\n\n        Returns:\n            str (str): The path to the saved image.\n        \"\"\"\n\n\n        if y is not None:\n             # Convert label data type from float to integer\n            arr_concat=np.concatenate((X, y.reshape(y.shape[0],1)), axis=1)\n            # Create a Pandas dataframe using the above array\n            df=pd.DataFrame(arr_concat, columns=['x', 'y', 'z', 'label'])\n            # Convert label data type from float to integer\n            df['label'] = df['label'].astype(int)\n            # Finally, sort the dataframe by label\n            df.sort_values(by='label', axis=0, ascending=True, inplace=True)\n\n            # Create a 3D graph\n            fig = px.scatter_3d(df, x='x', y='y', z='z', color=df['label'].astype(str), height=900, width=950, color_discrete_sequence=px.colors.qualitative.Vivid)\n\n        else:\n            # Create a Pandas dataframe using the above array\n            df = pd.DataFrame(X, columns=[\"x\", \"y\", \"z\"])\n\n            # Create a 3D graph\n            fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", height=900, width=950)\n\n\n        # Update chart looks\n        fig.update_layout(\n            title_text=\"{type}\",\n            showlegend=True,\n            legend=dict(orientation=\"h\", yanchor=\"top\", y=0, xanchor=\"center\", x=0.5),\n            scene_camera=dict(\n                up=dict(x=0, y=0, z=1),\n                center=dict(x=0, y=0, z=-0.1),\n                eye=dict(x=1.5, y=-1.4, z=0.5),\n            ),\n            margin=dict(l=0, r=0, b=0, t=0),\n            scene=dict(\n                xaxis=dict(\n                    backgroundcolor=\"white\",\n                    color=\"black\",\n                    gridcolor=\"#f0f0f0\",\n                    title_font=dict(size=10),\n                    tickfont=dict(size=10),\n                ),\n                yaxis=dict(\n                    backgroundcolor=\"white\",\n                    color=\"black\",\n                    gridcolor=\"#f0f0f0\",\n                    title_font=dict(size=10),\n                    tickfont=dict(size=10),\n                ),\n                zaxis=dict(\n                    backgroundcolor=\"lightgrey\",\n                    color=\"black\",\n                    gridcolor=\"#f0f0f0\",\n                    title_font=dict(size=10),\n                    tickfont=dict(size=10),\n                ),\n            ),\n        )\n        # Update marker size\n        fig.update_traces(marker=dict(size=3, line=dict(color=\"black\", width=0.1)))\n\n        path = f\"{self._data_path}/cluster_img.png\"\n        fig.write_image(path)\n\n        return path\n\n    def cluster_analysis(self, z : torch.tensor):\n\"\"\"\n        Applies the UMAP dimensionality reduction technique and HDBSCAN clustering algorithm to input data z, and\n        plots a 3D scatter plot of the reduced data with consistent colors for the same label.\n\n        Args:\n            z (torch.tensor): tensor The input data to cluster. It must be a PyTorch tensor.\n\n        Returns:\n            dict (dict): A dictionary containing the path to the image file.\n        \"\"\" \n\n        #if z is too large, reduce it to n samples\n        if z.shape[0] &gt; self.umap_input:\n            # choose 1000 random samples\n            z = z[np.random.choice(z.shape[0], self.umap_input, replace=False), :]\n\n        #print('clustering...')\n\n        z_umap = self.umap_reducer.fit_transform(z.detach().numpy())\n\n        # Apply HDBSCAN\n        clusterer = self.hdb.fit(z_umap)\n\n        # get the number of clusters \n        res = np.array(clusterer.labels_) \n        unique_res = np.unique(res) \n\n        # make a color palette with seaborn.\n        color_palette = sns.color_palette('flare', len(unique_res))\n\n\n        cluster_colors = [color_palette[x] if x &gt;= 0\n                        else (0.5, 0.5, 0.5)\n                        for x in clusterer.labels_]\n\n        cluster_member_colors = [sns.desaturate(x, p) for x, p in\n                                zip(cluster_colors, clusterer.probabilities_)]\n\n\n        path = self.cluster_plot(z_umap, clusterer.labels_)\n\n        return {'UMAP_img': path}\n\n    def calculate_differences(self, x: pd.DataFrame, xhat:pd.DataFrame):\n\"\"\"\n        Calculates the differences between two Pandas DataFrames `x` and `xhat`.\n\n        Args:\n            x (pd.DataFrame): A DataFrame of shape (num_samples, 52) representing the original data.\n            xhat (pd.DataFrame): A DataFrame of shape (num_samples, 52) representing the reconstructed data.\n\n        Returns:\n            (pd.DataFrame): A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.\n        \"\"\"\n\n        # Make a copy of xhat\n        xhat_0 = xhat.copy()\n        xhat_0['joint'] = 0\n\n        diff = x - xhat_0 \n        diff = diff.sort_values(by=[diff.index.name, 'joint'])\n\n        # Set the labels based on the joint number\n        diff['labels'] = diff['joint'].map(self.joint_labels)\n\n        # Bin the data based on the 'index'\n        bins = pd.cut(diff.index, bins=20)\n        diff['bins'] = bins\n\n        diff = diff.groupby(['labels', 'bins']).median().reset_index()\n\n        return diff\n\n    def plot_xyz_heatmap(self, diff: pd.DataFrame):\n\"\"\"\n        Plots separate heatmaps for the differences in x, y, and z coordinates.\n\n        Args:\n            diff (pd.DataFrame): A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.\n\n        Returns:\n            dict (dict): A dictionary containing the path to the image file.\n        \"\"\"\n\n        columns = [\"x\", \"y\", \"z\"]\n        fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\n        for i, col in enumerate(columns):\n            heatmap_data = diff.pivot_table(index=\"labels\", columns=\"bins\", values=col)\n            im = axes[i].imshow(heatmap_data, cmap=\"viridis\")\n            axes[i].set_title(f\"Difference btw {col.upper()} (Median)\")\n            axes[i].set_xlabel(\"Time\")\n            axes[i].set_ylabel(\"Joint\")\n            plt.colorbar(im, ax=axes[i], shrink=0.6)\n\n        # Save the plot as a PNG file\n        path = f\"{self._data_path}/xyz_recreation_img.png\"\n\n        plt.tight_layout()\n        plt.savefig(path)\n\n        return {'xyz_diff_img': path}\n\n\n    def plot_hip_distance_heatmap(self, diff):\n\"\"\"\n        Plots a heatmap for the differences in distances between each joint.\n\n        Args:\n            diff (pd.DataFrame): A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.\n\n        Returns:\n            dict (dict): A dictionary containing the path to the image file.\n        \"\"\"\n\n        heatmap_data = diff.pivot_table(index=\"labels\", columns=\"bins\", values=\"dist_hip\")\n        fig, ax = plt.subplots(figsize=(10, 6))\n        im = ax.imshow(heatmap_data, cmap=\"viridis\")\n        ax.set_title(\"Difference btw Distances (Median)\")\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Joint\")\n        plt.colorbar(im, ax=ax, shrink=0.6)\n        plt.tight_layout()\n\n        # Save the figure\n        path = f\"{self._data_path}/dist_recreation_img.png\"\n\n        plt.savefig(path)\n\n        return {'dist_diff_img': path}\n\n    def plot_joints(self, original_df, recreated_df, indexes):\n\"\"\"\n        Create a side-by-side comparison of 3D plots for each index with its associated joints\n        using the original and recreated dataframes.\n\n        Args:\n            original_df (pandas DataFrame): The original dataframe with columns [index, joint, x, y, z]\n            recreated_df (pandas DataFrame): The recreated dataframe with columns [index, joint, x, y, z]\n            indexes (list): A list of indexes to plot\n\n        Returns:\n            dict (dict) : A dictionary containing the path to the image file.\n        \"\"\"\n\n        # todo: add labels to the scatter plot\n        # todo: if there are landmarks, add them to the plot ? \n\n        n_indexes = len(indexes)\n\n        # Change the labels of the joints to match the ones in the original dataset\n        original_df['label'] = original_df['joint'].map(self.joint_labels)\n        recreated_df['label'] = recreated_df['joint'].map(self.joint_labels)\n\n        original_df['color'] = original_df['joint'].map(self.joint_colors)\n        recreated_df['color'] = recreated_df['joint'].map(self.joint_colors)\n\n        indexes_list = list(set(indexes.tolist())) \n\n        original_df = original_df.loc[indexes_list] \n        recreated_df = recreated_df.loc[indexes_list]\n\n        # Make index a column\n        original_df['index'] = original_df.index\n        recreated_df['index'] = recreated_df.index\n\n        # Group the data by index\n        original_grouped_index = original_df.groupby('index')\n        recreated_grouped_index = recreated_df.groupby('index')\n\n\n        # Create a figure with n_timepoints rows and one column of subplots\n        fig, axs = plt.subplots(n_indexes, 2, sharex=False, subplot_kw={'projection': '3d'}, figsize=(10, 5 * n_indexes))\n\n\n        # Iterate over each group and create a 3D plot\n        for row, (grouped_orig, grouped_recr) in enumerate(zip(original_grouped_index, recreated_grouped_index)):\n            # Plot each joint\n\n            jnt_data_orig = grouped_orig[1]\n            jnt_data_recr = grouped_recr[1]\n\n            axs[row][0].scatter(jnt_data_orig['x'], jnt_data_orig['y'], jnt_data_orig['z'], label='original' , c=jnt_data_orig['color'])\n            axs[row][1].scatter(jnt_data_recr['x'], jnt_data_recr['y'], jnt_data_recr['z'], label='recreated', c=jnt_data_orig['color'])\n\n\n            axs[row][0].set_title('Timepoint ' + str(row+1) + ' Original')\n            axs[row][1].set_title('Recreated ' + str(row+1))\n\n\n              # Plot lines between connected joints using joint_graph\n            for edge in self.joint_graph:\n                # Get coordinates of connected joints for the original data\n                joint1_orig = jnt_data_orig.loc[jnt_data_orig['label'] == edge[0]]\n                joint2_orig = jnt_data_orig.loc[jnt_data_orig['label'] == edge[1]]\n\n                # Get coordinates of connected joints for the recreated data\n                joint1_recr = jnt_data_recr.loc[jnt_data_recr['label'] == edge[0]]\n                joint2_recr = jnt_data_recr.loc[jnt_data_recr['label'] == edge[1]]\n\n                # Plot lines between connected joints in the original data\n                axs[row][0].plot([joint1_orig['x'], joint2_orig['x']],\n                                [joint1_orig['y'], joint2_orig['y']],\n                                [joint1_orig['z'], joint2_orig['z']],\n                                c='k', linestyle='-', linewidth=0.8)\n\n                # Plot lines between connected joints in the recreated data\n                axs[row][1].plot([joint1_recr['x'], joint2_recr['x']],\n                                [joint1_recr['y'], joint2_recr['y']],\n                                [joint1_recr['z'], joint2_recr['z']],\n                                c='k', linestyle='-', linewidth=0.8)\n\n\n                # remove the extra space between subplots\n        fig.tight_layout()\n\n        # save the figure\n        path = f\"{self._data_path}/3d_joints_img.png\"\n        fig.savefig(path)\n\n        return {'3d_img': path}\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.__call__","title":"<code>__call__(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for plotting.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>@abstractmethod\ndef __call__(self, **kwargs) -&gt; None:\n\"\"\"\n    Abstract method for plotting.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.__init__","title":"<code>__init__(data_path='None', label_path='None', min_cluster_size=10, umap_interval=5, umap_input=1000)</code>","text":"<p>Initializes the Plot class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the directory where the plots will be saved. Default is 'None'.</p> <code>'None'</code> <code>label_path</code> <code>str</code> <p>The path to the file containing the joint labels. Default is 'None'.</p> <code>'None'</code> <code>min_cluster_size</code> <code>int</code> <p>The minimum number of samples in a cluster. Default is 10.</p> <code>10</code> <code>umap_interval</code> <code>int</code> <p>The interval for applying UMAP to the data. Default is 5.</p> <code>5</code> <code>umap_input</code> <code>int</code> <p>The number of input samples for UMAP. Default is 1000.</p> <code>1000</code> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def __init__(self, data_path: str = 'None', label_path: str = 'None', min_cluster_size: int = 10, umap_interval : int = 5, umap_input: int = 1000) -&gt; None:\n\"\"\"\n    Initializes the Plot class.\n\n    Args:\n        data_path (str): The path to the directory where the plots will be saved. Default is 'None'.\n        label_path (str): The path to the file containing the joint labels. Default is 'None'.\n        min_cluster_size (int): The minimum number of samples in a cluster. Default is 10.\n        umap_interval (int): The interval for applying UMAP to the data. Default is 5.\n        umap_input (int): The number of input samples for UMAP. Default is 1000.\n    \"\"\"\n\n    self._data_path = data_path\n    self._label_path = label_path\n    self.umap_interval = umap_interval \n    self.num_joints = 13\n    self.umap_input = umap_input\n\n    self.transform = transforms.Compose([transforms.ToTensor()])\n    self.hdbscan = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # check if the datapath exists if not create it\n    self._data_path = os.path.join(self._data_path, 'plots')\n\n    if not os.path.exists(self._data_path):\n        os.makedirs(self._data_path)\n\n\n    # load the labels\n    with open(self._label_path, 'rb') as f:\n        self.joint_labels = pickle.load(f)\n\n    if self.num_joints == 13:\n        self.joint_colors = {i: color for i, color in enumerate([\"lightcoral\", \"coral\", \"sienna\", \"plum\", \"tomato\", \"darkslateblue\", \"salmon\", \"seagreen\", \"mediumspringgreen\", \"springgreen\", \"mediumvioletred\", \"deeppink\", \"greenyellow\"])}\n        self.joint_graph = [[\"neck\",\"head\"],[\"neck\",\"RShoulder\"],[\"neck\",\"Lshoulder\"],[\"neck\",\"hip\"],[\"head\",\"nose\"],[\"RShoulder\",\"RHand\"],[\"Lshoulder\",\"Lhand\"],[\"hip\",\"RKnee\"],[\"hip\",\"LKnee\"],[\"hip\",\"tail\"],[\"LKnee\",\"Lfoot\"],[\"RKnee\",\"RFoot\"]]\n    else:\n        raise ValueError(\"Number of joints not supported\")\n\n    self.umap_reducer = UMAP(\n        n_neighbors=15,  # default 15, The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n        n_components=3,  # default 2, The dimension of the space to embed into.\n        metric=\"euclidean\",  # default 'euclidean', The metric to use to compute distances in high dimensional space.\n        n_epochs=3,  # default None, The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings.\n        learning_rate=1.0,  # default 1.0, The initial learning rate for the embedding optimization.\n        init=\"spectral\",  # default 'spectral', How to initialize the low dimensional embedding. Options are: {'spectral', 'random', A numpy array of initial embedding positions}.\n        min_dist=0.1,  # default 0.1, The effective minimum distance between embedded points.\n        spread=1.0,  # default 1.0, The effective scale of embedded points. In combination with ``min_dist`` this determines how clustered/clumped the embedded points are.\n    )\n\n\n    self.hdb = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n        gen_min_span_tree=False, leaf_size=100,\n        metric='euclidean', min_samples=None, p=None, min_cluster_size=10)\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.calculate_differences","title":"<code>calculate_differences(x, xhat)</code>","text":"<p>Calculates the differences between two Pandas DataFrames <code>x</code> and <code>xhat</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>pd.DataFrame</code> <p>A DataFrame of shape (num_samples, 52) representing the original data.</p> required <code>xhat</code> <code>pd.DataFrame</code> <p>A DataFrame of shape (num_samples, 52) representing the reconstructed data.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def calculate_differences(self, x: pd.DataFrame, xhat:pd.DataFrame):\n\"\"\"\n    Calculates the differences between two Pandas DataFrames `x` and `xhat`.\n\n    Args:\n        x (pd.DataFrame): A DataFrame of shape (num_samples, 52) representing the original data.\n        xhat (pd.DataFrame): A DataFrame of shape (num_samples, 52) representing the reconstructed data.\n\n    Returns:\n        (pd.DataFrame): A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.\n    \"\"\"\n\n    # Make a copy of xhat\n    xhat_0 = xhat.copy()\n    xhat_0['joint'] = 0\n\n    diff = x - xhat_0 \n    diff = diff.sort_values(by=[diff.index.name, 'joint'])\n\n    # Set the labels based on the joint number\n    diff['labels'] = diff['joint'].map(self.joint_labels)\n\n    # Bin the data based on the 'index'\n    bins = pd.cut(diff.index, bins=20)\n    diff['bins'] = bins\n\n    diff = diff.groupby(['labels', 'bins']).median().reset_index()\n\n    return diff\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.cluster_analysis","title":"<code>cluster_analysis(z)</code>","text":"<p>Applies the UMAP dimensionality reduction technique and HDBSCAN clustering algorithm to input data z, and plots a 3D scatter plot of the reduced data with consistent colors for the same label.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>torch.tensor</code> <p>tensor The input data to cluster. It must be a PyTorch tensor.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the path to the image file.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def cluster_analysis(self, z : torch.tensor):\n\"\"\"\n    Applies the UMAP dimensionality reduction technique and HDBSCAN clustering algorithm to input data z, and\n    plots a 3D scatter plot of the reduced data with consistent colors for the same label.\n\n    Args:\n        z (torch.tensor): tensor The input data to cluster. It must be a PyTorch tensor.\n\n    Returns:\n        dict (dict): A dictionary containing the path to the image file.\n    \"\"\" \n\n    #if z is too large, reduce it to n samples\n    if z.shape[0] &gt; self.umap_input:\n        # choose 1000 random samples\n        z = z[np.random.choice(z.shape[0], self.umap_input, replace=False), :]\n\n    #print('clustering...')\n\n    z_umap = self.umap_reducer.fit_transform(z.detach().numpy())\n\n    # Apply HDBSCAN\n    clusterer = self.hdb.fit(z_umap)\n\n    # get the number of clusters \n    res = np.array(clusterer.labels_) \n    unique_res = np.unique(res) \n\n    # make a color palette with seaborn.\n    color_palette = sns.color_palette('flare', len(unique_res))\n\n\n    cluster_colors = [color_palette[x] if x &gt;= 0\n                    else (0.5, 0.5, 0.5)\n                    for x in clusterer.labels_]\n\n    cluster_member_colors = [sns.desaturate(x, p) for x, p in\n                            zip(cluster_colors, clusterer.probabilities_)]\n\n\n    path = self.cluster_plot(z_umap, clusterer.labels_)\n\n    return {'UMAP_img': path}\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.cluster_plot","title":"<code>cluster_plot(X, y=None)</code>","text":"<p>Plots a 3D scatter plot of the input data X with consistent colors for the same label.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy.array</code> <p>ndarray The input data to plot in 3D. It must have at least three columns.</p> required <code>y</code> <code>numpy.array</code> <p>ndarray The labels for the input data. If None, the labels will be generated automatically. (optional)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the saved image.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def cluster_plot(self, X: np.array, y :np.array = None):\n\"\"\"\n    Plots a 3D scatter plot of the input data X with consistent colors for the same label.\n\n    Args:\n        X (numpy.array): ndarray The input data to plot in 3D. It must have at least three columns.\n        y (numpy.array): ndarray The labels for the input data. If None, the labels will be generated automatically. (optional)\n\n    Returns:\n        str (str): The path to the saved image.\n    \"\"\"\n\n\n    if y is not None:\n         # Convert label data type from float to integer\n        arr_concat=np.concatenate((X, y.reshape(y.shape[0],1)), axis=1)\n        # Create a Pandas dataframe using the above array\n        df=pd.DataFrame(arr_concat, columns=['x', 'y', 'z', 'label'])\n        # Convert label data type from float to integer\n        df['label'] = df['label'].astype(int)\n        # Finally, sort the dataframe by label\n        df.sort_values(by='label', axis=0, ascending=True, inplace=True)\n\n        # Create a 3D graph\n        fig = px.scatter_3d(df, x='x', y='y', z='z', color=df['label'].astype(str), height=900, width=950, color_discrete_sequence=px.colors.qualitative.Vivid)\n\n    else:\n        # Create a Pandas dataframe using the above array\n        df = pd.DataFrame(X, columns=[\"x\", \"y\", \"z\"])\n\n        # Create a 3D graph\n        fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", height=900, width=950)\n\n\n    # Update chart looks\n    fig.update_layout(\n        title_text=\"{type}\",\n        showlegend=True,\n        legend=dict(orientation=\"h\", yanchor=\"top\", y=0, xanchor=\"center\", x=0.5),\n        scene_camera=dict(\n            up=dict(x=0, y=0, z=1),\n            center=dict(x=0, y=0, z=-0.1),\n            eye=dict(x=1.5, y=-1.4, z=0.5),\n        ),\n        margin=dict(l=0, r=0, b=0, t=0),\n        scene=dict(\n            xaxis=dict(\n                backgroundcolor=\"white\",\n                color=\"black\",\n                gridcolor=\"#f0f0f0\",\n                title_font=dict(size=10),\n                tickfont=dict(size=10),\n            ),\n            yaxis=dict(\n                backgroundcolor=\"white\",\n                color=\"black\",\n                gridcolor=\"#f0f0f0\",\n                title_font=dict(size=10),\n                tickfont=dict(size=10),\n            ),\n            zaxis=dict(\n                backgroundcolor=\"lightgrey\",\n                color=\"black\",\n                gridcolor=\"#f0f0f0\",\n                title_font=dict(size=10),\n                tickfont=dict(size=10),\n            ),\n        ),\n    )\n    # Update marker size\n    fig.update_traces(marker=dict(size=3, line=dict(color=\"black\", width=0.1)))\n\n    path = f\"{self._data_path}/cluster_img.png\"\n    fig.write_image(path)\n\n    return path\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.divide_tensor","title":"<code>divide_tensor(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Divides the tensor into the different parts (xyz, distance, landmark distance, etc)</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>@abstractmethod\ndef divide_tensor(self, **kwargs):\n\"\"\"\n    Divides the tensor into the different parts (xyz, distance, landmark distance, etc)\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.error_distribution_plot","title":"<code>error_distribution_plot(x, xhat)</code>","text":"<p>Plots a histogram of the error between the original and recreated data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>A tensor of shape (num_samples, 52/156) representing the original data.</p> required <code>xhat</code> <code>torch.Tensor</code> <p>A tensor of shape (num_samples, 52/156) representing the recreated data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dic</code> <p>A dictionary containing the name of the figure and the path to the figure.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def error_distribution_plot(self, x : torch.tensor , xhat : torch.tensor):\n\"\"\"\n    Plots a histogram of the error between the original and recreated data.\n\n    Args:\n        x (torch.Tensor): A tensor of shape (num_samples, 52/156) representing the original data.\n        xhat (torch.Tensor): A tensor of shape (num_samples, 52/156) representing the recreated data.\n\n    Returns:\n        dict (dic): A dictionary containing the name of the figure and the path to the figure.\n    \"\"\"\n\n    x = torch.reshape(x, (x.shape[0],  self.num_joints, -1))\n    xhat = torch.reshape(xhat, (xhat.shape[0],  self.num_joints, -1))\n\n\n    # Calculate the mean squared error (MSE) or mean absolute error (MAE) between the original and recreated data\n    errors = torch.mean((x - xhat) ** 2, dim=(1, 2))  # for MSE\n    # errors = torch.mean(torch.abs(original_data - recreated_data), dim=(1, 2))  # for MAE\n\n    fig, ax = plt.subplots()\n    ax.hist(errors.cpu().numpy(), bins=30, density=True, alpha=0.75)\n    ax.set_xlabel('Error Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Error Distribution between Original and Recreated Data')\n\n    # save the figure\n    path = os.path.join(self._data_path, 'error_distribution_fig.png')\n    fig.savefig(path)\n\n    img = Image.open(path)  \n    tensor_img = self.transform(img)\n\n    return {'error_distribution_fig': tensor_img}\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.plot_hip_distance_heatmap","title":"<code>plot_hip_distance_heatmap(diff)</code>","text":"<p>Plots a heatmap for the differences in distances between each joint.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>pd.DataFrame</code> <p>A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the path to the image file.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def plot_hip_distance_heatmap(self, diff):\n\"\"\"\n    Plots a heatmap for the differences in distances between each joint.\n\n    Args:\n        diff (pd.DataFrame): A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.\n\n    Returns:\n        dict (dict): A dictionary containing the path to the image file.\n    \"\"\"\n\n    heatmap_data = diff.pivot_table(index=\"labels\", columns=\"bins\", values=\"dist_hip\")\n    fig, ax = plt.subplots(figsize=(10, 6))\n    im = ax.imshow(heatmap_data, cmap=\"viridis\")\n    ax.set_title(\"Difference btw Distances (Median)\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Joint\")\n    plt.colorbar(im, ax=ax, shrink=0.6)\n    plt.tight_layout()\n\n    # Save the figure\n    path = f\"{self._data_path}/dist_recreation_img.png\"\n\n    plt.savefig(path)\n\n    return {'dist_diff_img': path}\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.plot_joints","title":"<code>plot_joints(original_df, recreated_df, indexes)</code>","text":"<p>Create a side-by-side comparison of 3D plots for each index with its associated joints using the original and recreated dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>original_df</code> <code>pandas DataFrame</code> <p>The original dataframe with columns [index, joint, x, y, z]</p> required <code>recreated_df</code> <code>pandas DataFrame</code> <p>The recreated dataframe with columns [index, joint, x, y, z]</p> required <code>indexes</code> <code>list</code> <p>A list of indexes to plot</p> required <p>Returns:</p> Type Description <p>dict (dict) : A dictionary containing the path to the image file.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def plot_joints(self, original_df, recreated_df, indexes):\n\"\"\"\n    Create a side-by-side comparison of 3D plots for each index with its associated joints\n    using the original and recreated dataframes.\n\n    Args:\n        original_df (pandas DataFrame): The original dataframe with columns [index, joint, x, y, z]\n        recreated_df (pandas DataFrame): The recreated dataframe with columns [index, joint, x, y, z]\n        indexes (list): A list of indexes to plot\n\n    Returns:\n        dict (dict) : A dictionary containing the path to the image file.\n    \"\"\"\n\n    # todo: add labels to the scatter plot\n    # todo: if there are landmarks, add them to the plot ? \n\n    n_indexes = len(indexes)\n\n    # Change the labels of the joints to match the ones in the original dataset\n    original_df['label'] = original_df['joint'].map(self.joint_labels)\n    recreated_df['label'] = recreated_df['joint'].map(self.joint_labels)\n\n    original_df['color'] = original_df['joint'].map(self.joint_colors)\n    recreated_df['color'] = recreated_df['joint'].map(self.joint_colors)\n\n    indexes_list = list(set(indexes.tolist())) \n\n    original_df = original_df.loc[indexes_list] \n    recreated_df = recreated_df.loc[indexes_list]\n\n    # Make index a column\n    original_df['index'] = original_df.index\n    recreated_df['index'] = recreated_df.index\n\n    # Group the data by index\n    original_grouped_index = original_df.groupby('index')\n    recreated_grouped_index = recreated_df.groupby('index')\n\n\n    # Create a figure with n_timepoints rows and one column of subplots\n    fig, axs = plt.subplots(n_indexes, 2, sharex=False, subplot_kw={'projection': '3d'}, figsize=(10, 5 * n_indexes))\n\n\n    # Iterate over each group and create a 3D plot\n    for row, (grouped_orig, grouped_recr) in enumerate(zip(original_grouped_index, recreated_grouped_index)):\n        # Plot each joint\n\n        jnt_data_orig = grouped_orig[1]\n        jnt_data_recr = grouped_recr[1]\n\n        axs[row][0].scatter(jnt_data_orig['x'], jnt_data_orig['y'], jnt_data_orig['z'], label='original' , c=jnt_data_orig['color'])\n        axs[row][1].scatter(jnt_data_recr['x'], jnt_data_recr['y'], jnt_data_recr['z'], label='recreated', c=jnt_data_orig['color'])\n\n\n        axs[row][0].set_title('Timepoint ' + str(row+1) + ' Original')\n        axs[row][1].set_title('Recreated ' + str(row+1))\n\n\n          # Plot lines between connected joints using joint_graph\n        for edge in self.joint_graph:\n            # Get coordinates of connected joints for the original data\n            joint1_orig = jnt_data_orig.loc[jnt_data_orig['label'] == edge[0]]\n            joint2_orig = jnt_data_orig.loc[jnt_data_orig['label'] == edge[1]]\n\n            # Get coordinates of connected joints for the recreated data\n            joint1_recr = jnt_data_recr.loc[jnt_data_recr['label'] == edge[0]]\n            joint2_recr = jnt_data_recr.loc[jnt_data_recr['label'] == edge[1]]\n\n            # Plot lines between connected joints in the original data\n            axs[row][0].plot([joint1_orig['x'], joint2_orig['x']],\n                            [joint1_orig['y'], joint2_orig['y']],\n                            [joint1_orig['z'], joint2_orig['z']],\n                            c='k', linestyle='-', linewidth=0.8)\n\n            # Plot lines between connected joints in the recreated data\n            axs[row][1].plot([joint1_recr['x'], joint2_recr['x']],\n                            [joint1_recr['y'], joint2_recr['y']],\n                            [joint1_recr['z'], joint2_recr['z']],\n                            c='k', linestyle='-', linewidth=0.8)\n\n\n            # remove the extra space between subplots\n    fig.tight_layout()\n\n    # save the figure\n    path = f\"{self._data_path}/3d_joints_img.png\"\n    fig.savefig(path)\n\n    return {'3d_img': path}\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.plot_landmark_distance_heatmap","title":"<code>plot_landmark_distance_heatmap(x_lmk_dist, xhat_lmk_dist)</code>","text":"<p>Plots the heatmap of the landmark distance.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def plot_landmark_distance_heatmap(self, x_lmk_dist, xhat_lmk_dist):\n\"\"\"\n    Plots the heatmap of the landmark distance. \n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.plot_xyz_heatmap","title":"<code>plot_xyz_heatmap(diff)</code>","text":"<p>Plots separate heatmaps for the differences in x, y, and z coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>pd.DataFrame</code> <p>A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the path to the image file.</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def plot_xyz_heatmap(self, diff: pd.DataFrame):\n\"\"\"\n    Plots separate heatmaps for the differences in x, y, and z coordinates.\n\n    Args:\n        diff (pd.DataFrame): A DataFrame containing the median differences between the original and reconstructed data, grouped by labels and bins.\n\n    Returns:\n        dict (dict): A dictionary containing the path to the image file.\n    \"\"\"\n\n    columns = [\"x\", \"y\", \"z\"]\n    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n\n    for i, col in enumerate(columns):\n        heatmap_data = diff.pivot_table(index=\"labels\", columns=\"bins\", values=col)\n        im = axes[i].imshow(heatmap_data, cmap=\"viridis\")\n        axes[i].set_title(f\"Difference btw {col.upper()} (Median)\")\n        axes[i].set_xlabel(\"Time\")\n        axes[i].set_ylabel(\"Joint\")\n        plt.colorbar(im, ax=axes[i], shrink=0.6)\n\n    # Save the plot as a PNG file\n    path = f\"{self._data_path}/xyz_recreation_img.png\"\n\n    plt.tight_layout()\n    plt.savefig(path)\n\n    return {'xyz_diff_img': path}\n</code></pre>"},{"location":"Model/plotting/plotting/#src.VAE.plots.plot_parent.Plot.recreation_analysis","title":"<code>recreation_analysis(x_xyz, xhat_xyz, x_dist, xhat_dist, x_lmk_dist=None, xhat_lmk_dist=None, z=None, n_timepoints=2, current_epoch=0)</code>","text":"<p>Compares the original data with the reconstructed data</p> <p>Parameters:</p> Name Type Description Default <code>x_xyz</code> <code>Pandas.df</code> <p>original data xyz values</p> required <code>xhat_xyz</code> <code>Pandas.df</code> <p>reconstructed data xyz values</p> required <code>x_dist</code> <code>Pandas.df</code> <p>original data distance from hip values  </p> required <code>xhat_dist</code> <code>Pandas.df</code> <p>reconstructed data distance from hip values</p> required <code>x_lmk_dist</code> <code>Pandas.df</code> <p>(optional: only for Landmark Version) original data distance from landmarks values</p> <code>None</code> <code>xhat_lmk_dist</code> <code>Pandas.df</code> <p>(optional: only for Landmark Version) reconstructed data distance from landmarks values</p> <code>None</code> <code>z</code> <code>torch.tensor</code> <p>(optional: only for Landmark Version) tensor of the model's encoded latent variables </p> <code>None</code> <code>n_timepoints</code> <code>int</code> <p>number of random samples to compare (pose only)</p> <code>2</code> <code>current_epoch</code> <code>int</code> <p>current epoch</p> <code>0</code> <p>Returns:</p> Name Type Description <code>fig_dic</code> <code>dict</code> <p>dictionary with tensors of the figures</p> Source code in <code>src/VAE/plots/plot_parent.py</code> <pre><code>def recreation_analysis(self, x_xyz : pd.DataFrame,\n                        xhat_xyz: pd.DataFrame,  \n                        x_dist: pd.DataFrame, \n                        xhat_dist: pd.DataFrame,\n                        x_lmk_dist=None,\n                        xhat_lmk_dist=None, \n                        z=None,\n                        n_timepoints=2, \n                        current_epoch=0):\n\"\"\"\n    Compares the original data with the reconstructed data\n\n    Args:\n        x_xyz (Pandas.df): original data xyz values\n        xhat_xyz (Pandas.df): reconstructed data xyz values\n        x_dist (Pandas.df):  original data distance from hip values  \n        xhat_dist (Pandas.df): reconstructed data distance from hip values\n        x_lmk_dist (Pandas.df): (optional: only for Landmark Version) original data distance from landmarks values\n        xhat_lmk_dist (Pandas.df): (optional: only for Landmark Version) reconstructed data distance from landmarks values\n        z (torch.tensor): (optional: only for Landmark Version) tensor of the model's encoded latent variables \n        n_timepoints (int): number of random samples to compare (pose only)\n        current_epoch (int): current epoch\n\n    Returns: \n        fig_dic (dict): dictionary with tensors of the figures\n    \"\"\"\n\n    # choose n random indexes to plot \n    if n_timepoints &gt; len(x_xyz.index.unique()):\n        n_timepoints = len(x_xyz.index.unique())\n\n    indexes = np.random.choice(len(x_xyz.index.unique()), n_timepoints, replace=False)\n\n    # plot the heatmaps\n\n    xyz_diff = self.calculate_differences(x_xyz, xhat_xyz)\n    hip_dist_diff = self.calculate_differences(x_dist, xhat_dist)\n\n    heatmaps_xyz_fig = self.plot_xyz_heatmap(xyz_diff)\n    heatmap_dist_fig = self.plot_hip_distance_heatmap(hip_dist_diff)\n\n    heatmap_dist_lmk_fig = None\n\n    if x_lmk_dist is not None:\n        # todo ! \n        # lmk_dist_diff = self.calculate_differences(x_lmk_dist, xhat_lmk_dist)\n        # heatmap_dist_lmk_fig = self.plot_landmark_distance_heatmap(lmk_dist_diff)\n        heatmap_dist_lmk_fig = None\n\n    # close the figures\n    matplotlib.pyplot.close()\n\n    # plot the joints in 3D space \n    joints_fig = self.plot_joints(original_df = x_xyz,recreated_df= xhat_xyz, indexes= indexes)\n\n    # close the figures\n    matplotlib.pyplot.close()\n\n    if z is not None and (current_epoch % self.umap_interval  == 1 or current_epoch == 1):\n        # plot the clusters using umap\n        cluster_fig = self.cluster_analysis(z)\n\n        # close the figures\n        matplotlib.pyplot.close()\n\n        # join the dictionarys into one\n        fig_dict = {**heatmaps_xyz_fig, **heatmap_dist_fig, **heatmap_dist_lmk_fig, **joints_fig, **cluster_fig} if heatmap_dist_lmk_fig is not None else {**heatmaps_xyz_fig, **heatmap_dist_fig, **joints_fig, **cluster_fig}\n    else:\n        fig_dict = {**heatmaps_xyz_fig, **heatmap_dist_fig, **heatmap_dist_lmk_fig, **joints_fig} if heatmap_dist_lmk_fig is not None else {**heatmaps_xyz_fig, **heatmap_dist_fig, **joints_fig}\n\n    tensor_dict = {}\n\n    for fig in fig_dict:\n        # open the picture and convert it to tensor\n\n        img = Image.open(fig_dict[fig])  \n        tensor_img = self.transform(img)\n        tensor_dict[fig] = tensor_img\n\n    return tensor_dict\n</code></pre>"},{"location":"exploration/exploration/","title":"Exploration MonkeyVideo","text":"<p>This module contains functions to plot 3D scatter plots and connecting lines from a pandas DataFrame.</p> This module contains the following constants <ul> <li>JOINT_COLORS (dict): A dictionary mapping joint names to their corresponding color values.</li> <li>LIMB_COLORS (dict): A dictionary mapping limb names to their corresponding color values.</li> <li>CONNECTIONS (list): A list of lists containing pairs of joints to connect.</li> </ul> This module contains the following functions <ul> <li>connecting_lines(df, connections): A function that takes a pandas DataFrame with all markers and a list of connections, and returns a dictionary of concatenated DataFrames.</li> <li>plot_video(df, time, file_name): A function that plots a 3D scatter plot with connecting lines between joints for each frame in a given time range and saves the animation as a GIF file.</li> <li>plot_frame(df, con_df, i, frames): A function that plots a single frame of a 3D scatter plot with connecting lines.</li> </ul>"},{"location":"exploration/exploration/#src.exploration.monkey_video.connecting_lines","title":"<code>connecting_lines(df, connections)</code>","text":"<p>Concatenates specified columns from a pandas dataframe <code>df</code> and returns a dictionary <code>c_dict</code> with keys as 'column_1-column_2' (e.g. 'A-B') and values as the concatenated dataframe for each connection.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The dataframe that contains all the joint markers and their corresponding x, y, and z coordinates.</p> required <code>connections</code> <code>list</code> <p>A list of connections (each represented as a tuple of two column names) to be concatenated</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are in the format 'column_1-column_2' and values are concatenated dataframes.</p> Source code in <code>src/exploration/monkey_video.py</code> <pre><code>def connecting_lines(df : pandas.DataFrame, connections : list) -&gt; dict:\n\"\"\"\n    Concatenates specified columns from a pandas dataframe `df` and returns a dictionary `c_dict` with keys as 'column_1-column_2' (e.g. 'A-B') and values as the concatenated dataframe for each connection.\n\n    Parameters:\n        df (pandas.DataFrame): The dataframe that contains all the joint markers and their corresponding x, y, and z coordinates.\n        connections (list): A list of connections (each represented as a tuple of two column names) to be concatenated\n\n    Returns:\n        dict (dict): A dictionary where keys are in the format 'column_1-column_2' and values are concatenated dataframes.\n    \"\"\"\n\n    c_dict = {}\n\n    for connection in connections:\n        c_dict[f'{connection[0]}-{connection[1]}'] = pd.concat([df[connection[0]], df[connection[1]]], axis=1).drop(columns=['label'])\n\n    return c_dict\n</code></pre>"},{"location":"exploration/exploration/#src.exploration.monkey_video.plot_frame","title":"<code>plot_frame(df=None, con_df=None, i=None, frames=None)</code>","text":"<p>Plots a single frame of 3D scatter plot of the coordinates and connections between them.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.Dataframe</code> <p>A pandas DataFrame containing the coordinates for a single frame.</p> <code>None</code> <code>con_df</code> <code>pandas.Dataframe</code> <p>A pandas DataFrame containing the connections between the coordinates.</p> <code>None</code> <code>i</code> <code>int</code> <p>An integer indicating the index of the frame.</p> <code>None</code> <code>frames</code> <code>int</code> <p>An integer indicating the total number of frames in the animation.</p> <code>None</code> <p>Returns:</p> Type Description <p>A plotly Figure object containing the scatter plot of the coordinates and connections.</p> Side effects <ul> <li>Converts the coordinate points and connection points to NumPy arrays.</li> <li>Creates a 3D scatter plot of the coordinates and connections using the Plotly library.</li> <li>Logs progress and errors.</li> </ul> Source code in <code>src/exploration/monkey_video.py</code> <pre><code>def plot_frame(df : pandas.DataFrame = None, con_df : pandas.DataFrame = None, i : int= None, frames : int = None):\n\"\"\"\n    Plots a single frame of 3D scatter plot of the coordinates and connections between them.\n\n    Args:\n        df (pandas.Dataframe): A pandas DataFrame containing the coordinates for a single frame.\n        con_df (pandas.Dataframe): A pandas DataFrame containing the connections between the coordinates.\n        i (int): An integer indicating the index of the frame.\n        frames (int): An integer indicating the total number of frames in the animation.\n\n    Returns:\n        A plotly Figure object containing the scatter plot of the coordinates and connections.\n\n    Side effects:\n        - Converts the coordinate points and connection points to NumPy arrays.\n        - Creates a 3D scatter plot of the coordinates and connections using the Plotly library.\n        - Logs progress and errors.\n\n    \"\"\"\n\n\n    # convert the points to numpy \n    df = df.loc[[i]]\n    con_df = con_df.loc[[i]]\n\n    con_df['x'] = con_df[['x1', 'x2']].values.tolist()\n    con_df['y'] = con_df[['y1', 'y2']].values.tolist()\n    con_df['z'] = con_df[['z1', 'z2']].values.tolist()\n\n    # create the figure\n    # with a specific size\n    fig = plt.figure(figsize=(10, 10))\n\n    ax = plt.axes(projection='3d')\n\n    # Set names for axis \n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_zlabel('Z-axis')\n\n    fig = px.scatter_3d(df, x='x', y='y', z='z',  color='color')\n    fig.update_traces(marker=dict(size=4))\n\n    for index, row in con_df.iterrows():\n        tem = pd.DataFrame(row).T\n\n        x = tem['x'].values[0]\n        y = tem['y'].values[0]\n        z = tem['z'].values[0]\n        label = tem['label'].values[0]\n\n        fig.add_trace(go.Scatter3d(x=x, y=y,z=z,mode='lines'))\n\n    fig.update_layout(\n    scene = dict(\n        xaxis = dict(nticks=10, range=[-5,5],),\n        yaxis = dict(nticks=10, range=[-5,5],),\n        zaxis = dict(nticks=10, range=[-5,5],),),\n        height = 800,\n        width=800)\n    fig.update_layout(showlegend=False)\n\n    ax.set_xlim(-5,5)\n    ax.set_ylim(-5,5)\n    ax.set_zlim(-5,5)\n\n\n    # adjust the plot position automatically\n    plt.tight_layout()\n\n\n    return fig\n</code></pre>"},{"location":"exploration/exploration/#src.exploration.monkey_video.plot_video","title":"<code>plot_video(df, time, file_name)</code>","text":"<p>Generates a GIF of a 3D skeleton animation using a dataframe <code>df</code> containing x, y, and z position data for each joint in the monkey skeleton. The GIF covers a <code>time</code>-second time window and is saved under the  <code>file_name</code> provided in the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The dataframe containing the position data for each joint in the skeleton</p> required <code>time</code> <code>int</code> <p>The time in seconds to cover in the generated GIF</p> required <code>file_name</code> <code>str</code> <p>The name to use for the generated GIF file</p> required Side Effects <ul> <li>Creates a directory 'gifs' and 'pics' in the output directory if they don't already exist.</li> <li>Saves a GIF file and individual PNG frames for each time point in the <code>time</code>-second window in the 'gifs' and 'pics' directories, respectively.</li> </ul> Source code in <code>src/exploration/monkey_video.py</code> <pre><code>def plot_video(df : pd.DataFrame, time: int, file_name: str):\n\"\"\"\n    Generates a GIF of a 3D skeleton animation using a dataframe `df` containing x, y, and z position data\n    for each joint in the monkey skeleton. The GIF covers a `time`-second time window and is saved under the \n    `file_name` provided in the output directory.\n\n    Parameters:\n        df (pandas.DataFrame): The dataframe containing the position data for each joint in the skeleton\n        time (int): The time in seconds to cover in the generated GIF\n        file_name (str): The name to use for the generated GIF file\n\n    Side Effects:\n        - Creates a directory 'gifs' and 'pics' in the output directory if they don't already exist.\n        - Saves a GIF file and individual PNG frames for each time point in the `time`-second window\n        in the 'gifs' and 'pics' directories, respectively.\n\n    \"\"\"\n    #1. divide the dataframe into a dictionary of dataframes, one for each joint\n    dfs_dict = du.divide_markers(df)\n\n    #2. create a dictionary of lines connecting the joints\n    c_dict = connecting_lines(dfs_dict, CONNECTIONS)\n\n    # 3. concatinate all of the dfs \n    frames = []\n    for key in c_dict.keys():\n        temp_b = pd.DataFrame.from_dict(c_dict[key])\n        temp_b['labels'] = key\n        frames.append(temp_b)\n        plt.close('all')\n\n    connecting_lines_df = pd.concat(frames).sort_index()\n\n    # 4. rename the columns\n    connecting_lines_df.columns = ['x1', 'y1','z1','x2', 'y2', 'z2', 'label']\n\n    # 5. map the labels to a unique color value for each joint\n    connecting_lines_df.loc[:, 'color']  = connecting_lines_df['label'].map(LIMB_COLORS)\n\n    # map the labels to a unique color value for each joint\n    df.loc[:, 'color'] = df['label'].map(JOINT_COLORS)\n\n    # choose a random time point to start the gif \n    start_time = random.randint(0, df.index[-1] - time)\n    end_time = start_time + time\n\n    df = df[(df.index &gt;= start_time) &amp; (df.index &lt; end_time)]\n\n    # 6. create the folder to save the gif\n    gif_path = os.path.join(flags.FLAGS.output_directory, 'gifs')\n    gif_path = os.path.join(gif_path, file_name)\n\n    pic_path = os.path.join(flags.FLAGS.output_directory, 'pics')\n    pic_path = os.path.join(pic_path, file_name)\n\n    if not os.path.exists(gif_path):\n        os.makedirs(gif_path)\n\n    if not os.path.exists(pic_path):\n        os.makedirs(pic_path)\n\n\n    log.info(f\"Creating the gif for {file_name}\")\n    print(f\"Creating the gif for {file_name}\")\n    log.info(f\"save path: {gif_path}\")\n    log.info(f\"image path: {pic_path}\")\n\n    # 7. create and save the gif\n\n    with imageio.get_writer(f'{gif_path}/{start_time}_{end_time}.gif', mode='I',  loop=False) as writer:\n        for i in tqdm(range(start_time, end_time)):\n            fig = plot_frame(df, connecting_lines_df, i, frames)\n            fig.write_image(f'{pic_path}/{i}.png')\n            image = imageio.imread(f'{pic_path}/{i}.png')\n            writer.append_data(image)\n            matplotlib.pyplot.close()\n</code></pre>"},{"location":"exploration/main/","title":"Main","text":"<p>Main entry point to run the exploration pipeline.</p> <p>This module provides the functionality to run an exploration pipeline, which consists of a series of steps to visualize the original and pre-processed data. This helps to gain a better understanding of the data by creating various plots and visualizations.</p> <p>The module takes input, output, and log directory paths, as well as the log level, as command-line arguments. It performs a series of safety checks on the directories and sets up the required sub-folders. Then, it visualizes the data by generating different plots using the plots module.</p> Example <p>python3 exploration/main.py --input_directory=data --output_directory=data/plots --log_directory=exploration/logs --log_level=DEBUG</p> <p>Attributes:</p> Name Type Description <code>-</code> <code>video_files (int</code> <p>Number of random files which will be selected to make a gif </p> <code>-</code> <code>time (int</code> <p>Number of frames to plot in gif</p> <code>-</code> <code>log_directory (str</code> <p>Prefix for the log directory.</p> <code>-</code> <code>log_level (str</code> <p>Log level to use</p> <code>-</code> <code>n_files (int</code> <p>Number of random files which will be selected to plot</p> <code>-</code> <code>original_data_directory (str</code> <p>Directory of original data</p> <code>-</code> <code>output_directory (str</code> <p>Output file directory of processed data</p> <code>-</code> <code>stats_directory (str</code> <p>Directory where the stats are stored</p> <p>Example usage:</p> <p>python3 exploration/main.py --input_directory=src/data/original --output_directory=src/data/plots --log_directory=src/exploration/logs --log_level=DEBUG</p>"},{"location":"exploration/main/#src.exploration.main.main","title":"<code>main()</code>","text":"<p>Main function to run the exploration pipeline.</p> Source code in <code>src/exploration/main.py</code> <pre><code>def main():\n\"\"\"Main function to run the exploration pipeline.\"\"\"\n\n    # safety check on the input and output folder and log folder\n    # all of them should end with a slash\n    FLAGS.original_data_directory = setup_utils.slash_check(\n        FLAGS.original_data_directory\n    )\n    FLAGS.output_directory = setup_utils.slash_check(FLAGS.output_directory)\n    FLAGS.log_directory = setup_utils.slash_check(FLAGS.log_directory)\n    FLAGS.stats_directory = setup_utils.slash_check(FLAGS.stats_directory)\n\n    setup_utils.logger_setup()\n    log.info(f\"Running {__file__} with arguments: {sys.argv[1:]}\")\n\n    ##############################################\n    # Safety check on the input and output folder\n    ##############################################\n    setup_utils.create_folder(FLAGS.output_directory)\n    setup_utils.create_folder(FLAGS.log_directory)\n\n    setup_utils.safety_check(FLAGS.original_data_directory)\n    setup_utils.safety_check(FLAGS.output_directory)\n    setup_utils.safety_check(FLAGS.log_directory)\n    setup_utils.safety_check(FLAGS.stats_directory)\n\n    sub_folders = [\"boxplot\", \"outliers\", \"pose_img\", \"pose_video\", \"stats\"]\n\n    setup_utils.setup_subfolders(FLAGS.output_directory, sub_folders)\n\n    ##############################################\n    # Visualize the data\n    ##############################################\n\n    plots.outliers_percent(FLAGS.stats_directory)\n    plots.outliers_boxplot()\n    plots.monkey_video()\n    plots.outlier_event_plot()\n</code></pre>"},{"location":"exploration/stats/","title":"Exploration Stats","text":"<p>This module contains functions for calculating statistics on the data. It is used to calculate the percentage of outliers in the data and to calculate the mean and standard deviation for each file of the data.</p> <p>This module processes the data files from the given input directory using the command-line arguments provided. It does the following tasks:     - Checks for safety on the input, output and log directories     - Loads data from input directory     - Calculates stats and creates a json file with an overview of stats     - Converts data to tensor and saves it to the 'tensor' folder     - Saves the stats data in the form of a pandas dataframe to the 'stats' folder as a pickle file.</p> Usage <ul> <li>log_directory: Prefix for the log directory.</li> <li>log_level: Log level to use</li> <li>input_directory: Input file directory of data to process</li> <li>output_directory: Output file directory of processed data</li> </ul> Example <p>python my_file.py --input_directory=my_input_dir --output_directory=my_output_dir</p>"},{"location":"processing/data_processing/","title":"Data Processing","text":"<p>This module contains functions to preprocess and analyze 3D joint position data from motion capture systems.</p> Functions <ul> <li>center_hip(df): Centers all the other joints around the hip joint.</li> <li>calculate_distance(df): Calculates the distance between the hip joint and all other joints.</li> <li>check_hip_centered(df): Checks whether the hip joint has been correctly centered in the given DataFrame.</li> <li>add_landmark_location(df): Adds the coordinates of objects (barrels and feeders) to the DataFrame.</li> </ul>"},{"location":"processing/data_processing/#src.processing.data_processing.add_landmark_location","title":"<code>add_landmark_location(df)</code>","text":"<p>Adds the coordinates of the objects (4 barrels and 4 feeders) to the dataframe. </p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The dataframe to add the object coordinates to</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>The dataframe with the object coordinates added as columns to each row</p> Source code in <code>src/processing/data_processing.py</code> <pre><code>def add_landmark_location(df: pd.DataFrame):\n\"\"\"\n    Adds the coordinates of the objects (4 barrels and 4 feeders) to the dataframe. \n\n    Args:\n        df (pd.DataFrame): The dataframe to add the object coordinates to\n\n    Returns:\n        df (pd.DataFrame): The dataframe with the object coordinates added as columns to each row\n    \"\"\"\n\n    lmks = ['b1', 'b2', 'b3', 'b4', 'f1', 'f2', 'f3', 'f4']\n\n    coords = [\n        [1.8831099474237765, 2.2504857710896258, 3.1104950213839317],\n        [3.2684966975634477, 2.3323177565826185, 3.0727974914230116],\n        [-2.740061407095278, 3.0620530012108063, -2.0537183009440745],\n        [-3.090940351965614, 3.0070227700837804, -0.8690008794238412],\n        [-2.161960556268887, 2.428982849819521, 3.5207878102359627],\n        [3.868664285069503, -0.19594825889453213, 2.5292130833751956],\n        [2.5104482176851417, 3.281142061125232, -2.044059905334382],\n        [-3.5609489140305444, 1.7744324668075677, -1.8620419355950952]\n    ]\n\n    df_lmks = df.copy()\n\n    # calculate distance between each row and each coordinate\n    distances = cdist(df[['x', 'y', 'z']], coords)\n\n    # create new columns in dataframe with distances\n    for i in range(len(coords)):\n        df_lmks[f'distance_to_coord_{lmks[i]}'] = distances[:, i]\n\n    df_lmks.sort_index(inplace=True)\n\n    return df_lmks\n</code></pre>"},{"location":"processing/data_processing/#src.processing.data_processing.calculate_distance","title":"<code>calculate_distance(df)</code>","text":"<p>Calculates the distance between the hip and the rest of the joints</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame | pd.Series</code> <p>the dataframe to calculate distance for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>: the dataframe with the calculated distances</p> Source code in <code>src/processing/data_processing.py</code> <pre><code>def calculate_distance(df):\n\"\"\"\n    Calculates the distance between the hip and the rest of the joints\n\n    parameters:\n        df (pd.DataFrame|pd.Series): the dataframe to calculate distance for\n\n    returns:\n        df (pd.DataFrame):: the dataframe with the calculated distances\n    \"\"\"\n\n    df[\"distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n\n    return df\n</code></pre>"},{"location":"processing/data_processing/#src.processing.data_processing.center_hip","title":"<code>center_hip(df)</code>","text":"<p>Centers the other joints around the hip</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>the dataframe to center</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>the dataframe with the hip centered around the rest of the joints</p> Source code in <code>src/processing/data_processing.py</code> <pre><code>def center_hip(df: pd.DataFrame):\n\"\"\"\n    Centers the other joints around the hip\n\n    parameters:\n        df (pd.DataFrame): the dataframe to center\n\n    returns:\n        df (pd.DataFrame): the dataframe with the hip centered around the rest of the joints\n    \"\"\"\n\n    groups = df.groupby(\"label\")\n    hip = groups.get_group(\"hip\")\n    hip = hip.drop(columns=[\"label\"])\n\n    temp_df = pd.DataFrame()\n    for name, group in groups:\n        if name != \"hip\":\n            group = group.drop(columns=[\"label\"])\n\n            group['x'] = group['x'] - hip['x']\n            group['y'] = group['y'] - hip['y']\n            group['z'] = group['z'] - hip['z']\n\n            group[\"label\"] = name\n            temp_df = pd.concat([temp_df, group], axis=0)\n\n    # center the hip\n    hip['x'] = 0\n    hip['y'] = 0\n    hip['z'] = 0\n\n    hip[\"label\"] = \"hip\"\n    temp_df = pd.concat([temp_df, hip], axis=0)\n\n    return temp_df\n</code></pre>"},{"location":"processing/data_processing/#src.processing.data_processing.check_hip_centered","title":"<code>check_hip_centered(df)</code>","text":"<p>Checks if the hip joint has been centered correctly in the given DataFrame.</p> <p>The function groups the data by label, extracts the hip joint data, and then checks if the x, y, and z coordinates of the hip joint are all 0. If they are, the hip has been centered correctly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame | pd.Series</code> <p>A DataFrame containing joint data with a 'label' column.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the hip joint has been centered correctly, False otherwise.</p> Source code in <code>src/processing/data_processing.py</code> <pre><code>def check_hip_centered(df):\n\"\"\"\n    Checks if the hip joint has been centered correctly in the given DataFrame.\n\n    The function groups the data by label, extracts the hip joint data, and then checks if\n    the x, y, and z coordinates of the hip joint are all 0. If they are, the hip has been centered correctly.\n\n    Parameters:\n        df (pandas.DataFrame| pd.Series): A DataFrame containing joint data with a 'label' column.\n\n    Returns:\n        bool (bool): True if the hip joint has been centered correctly, False otherwise.\n    \"\"\"\n\n    groups = df.groupby(\"label\")\n    hip = groups.get_group(\"hip\")\n    hip = hip.drop(columns=[\"label\"])\n\n    if hip['x'].all() == 0 and hip['y'].all() == 0 and hip['z'].all() == 0:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"processing/file_processing/","title":"File Processing","text":"<p>This module contains functions for processing, preprocessing, and saving human joint data in the form of tensors.</p>"},{"location":"processing/file_processing/#src.processing.file_processing.concatenate_tensors_from_files","title":"<code>concatenate_tensors_from_files(input_dir, files)</code>","text":"<p>Concatenates all tensors in the given list of files in the given directory and returns the result. Excludes the file 'joint.pt' and prints the name of any file that contains NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>The directory containing the input files.</p> required <code>files</code> <code>List[str]</code> <p>A list of filenames to read tensors from.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: A concatenated tensor containing the data from all valid input files.</p> Source code in <code>src/processing/file_processing.py</code> <pre><code>def concatenate_tensors_from_files(input_dir: str, files):\n\"\"\"\n    Concatenates all tensors in the given list of files in the given directory and returns the result.\n    Excludes the file 'joint.pt' and prints the name of any file that contains NaN values.\n\n    Parameters:\n        input_dir (str): The directory containing the input files.\n        files (List[str]): A list of filenames to read tensors from.\n\n    Returns:\n        torch.Tensor: A concatenated tensor containing the data from all valid input files.\n\n    \"\"\"\n    all_tensors = []\n    for file in tqdm(files):\n        if file == \"joint.pt\":\n            continue\n        path = os.path.join(input_dir, file)\n        data = nn.load(path)\n        if nn.any(nn.isnan(data)):\n            print(file)\n        else:\n            all_tensors.append(data)\n\n    # concatenate tensors\n    tensor = nn.cat(all_tensors, dim=0)\n    return tensor\n</code></pre>"},{"location":"processing/file_processing/#src.processing.file_processing.join_files","title":"<code>join_files()</code>","text":"<p>Load and concatenate tensors from specified subdirectories, and save the resulting tensors in the same subdirectories as 'joint.pt' and 'joint_scaled.pt'.</p> side effects <p>saves the joint tensors to the folder as a tensor file with the joint.pt and joint_scaled.pt</p> Source code in <code>src/processing/file_processing.py</code> <pre><code>def join_files():\n\"\"\"\n    Load and concatenate tensors from specified subdirectories, and save the resulting tensors in\n    the same subdirectories as 'joint.pt' and 'joint_scaled.pt'.\n\n    side effects:\n        saves the joint tensors to the folder as a tensor file with the joint.pt and joint_scaled.pt\n\n    \"\"\"\n\n    subdirs = [f for f in os.listdir(flags.FLAGS.output_directory) if os.path.isdir(os.path.join(flags.FLAGS.output_directory, f))]\n    # load files\n\n    for subdir in tqdm(subdirs, desc=\"Processing subdirectories\"):\n        input_dir = os.path.join(FLAGS.output_directory, subdir)\n        files = os.listdir(input_dir)\n        tensor = concatenate_tensors_from_files(input_dir, files)\n        nn.save(tensor, f\"{input_dir}/joint.pt\")\n        scaled_tensor = (tensor.float() - nn.min(tensor).float()) / (nn.max(tensor).float() - nn.min(tensor).float())\n        nn.save(scaled_tensor, f\"{input_dir}/joint_scaled.pt\")\n</code></pre>"},{"location":"processing/file_processing/#src.processing.file_processing.process_file","title":"<code>process_file(file)</code>","text":"<p>Processes a given file by performing data preprocessing, interpolation, centering, and saving the data as tensors.</p> The function performs the following steps <ol> <li>Opens the original data file and converts it to a DataFrame.</li> <li>Interpolates the data to fill missing values.</li> <li>Adds landmark location information to the data.</li> <li>Decodes the label column back to its original values.</li> <li>Saves the data in various stages (without centering, with centering) as tensors.</li> <li>Centers the hip joint in the data.</li> <li>Checks if the hip has been centered correctly and exits if not.</li> <li>Saves the centered data as tensors.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The name of the file to process.</p> required Source code in <code>src/processing/file_processing.py</code> <pre><code>def process_file(file:  str):\n\"\"\"\n        Processes a given file by performing data preprocessing, interpolation, centering, and saving the data as tensors.\n\n        The function performs the following steps:\n            1. Opens the original data file and converts it to a DataFrame.\n            2. Interpolates the data to fill missing values.\n            3. Adds landmark location information to the data.\n            4. Decodes the label column back to its original values.\n            5. Saves the data in various stages (without centering, with centering) as tensors.\n            6. Centers the hip joint in the data.\n            7. Checks if the hip has been centered correctly and exits if not.\n            8. Saves the centered data as tensors.\n\n        Parameters:\n            file (str): The name of the file to process.\n\n    \"\"\"\n    log.info(f\"Processing file: {file}\")\n\n    df_numeric, mapping = du.open_original_to_df(file = file, path =  flags.FLAGS.input_directory, to_numeric= True)\n\n    log.info(\"Interpolating data\")\n    df, df_clean = du.interpolate(df_numeric)\n\n    df_landmark = add_landmark_location(df)\n    df_clean_landmark = add_landmark_location(df_clean)\n\n    dfs = {\n        # \"coordinates\": df,\n        \"coordinates_clean\": df_clean,\n        # \"landmark\": df_landmark,\n        \"landmark_clean\": df_clean_landmark,\n    }\n\n\n    for key, df in tqdm(dfs.items(), desc=\"Processing dataframes\"):\n\n        df[\"label\"] = df[\"label\"].map(mapping, na_action='ignore')\n\n        du.save_to_tensor(df = df, filename = file, type=key)\n\n        log.info(\"Centering data to hip\")\n        centered_df = center_hip(df)\n        df_centered = calculate_distance(df=centered_df)\n\n        if not check_hip_centered(df_centered):\n            log.info(\"Hip has not been centered\")\n            exit()\n\n        if key  == 'coordinates': \n            name = 'hip_centered'\n        elif key == 'coordinates_clean':\n            name = 'hip_centered_clean'\n        else: \n            name = key + 'hip_centered'\n\n        if key == list(dfs.keys())[0]:\n            du.save_to_tensor(df_centered, file, type=name, save_labels=True)\n        du.save_to_tensor(df_centered, file, type=f\"{name}\", save_labels=False)\n</code></pre>"},{"location":"processing/main/","title":"Main","text":"<p>Main entry point to run the preprocessing pipeline.</p> The preprocessing pipeline consists of the following steps <ol> <li>Load the data</li> <li>Visualize the data</li> <li>Preprocess the data (remove outliers, interpolate, center to hip, get distance between hip and rest of body)</li> <li>Save the data</li> </ol> The output folder will contain the following folders <ol> <li>coordinates: contains the processed data without hip centering</li> <li>hip_centered: contains the processed data with hip centering meaning that the hip is the origin of the coordinate system</li> <li>stats: contains the statistics of the data (mean, std, min, max, percentage_of_outliers)</li> </ol> <p>The shape of the data is (N, 13, 4) where N is the number of frames, 13 is the number of body parts and 4 is the x, y and z coordinates + the distance to the hip. Or  (N, 13, 8) where N is the number of frames, 13 is the number of body parts and 8 are the x, y and z coordinates, distance to each of the landmarks (4 barrels and 4 feeders) and distanc.e to the hip</p> Example <p>python3 src/preprocessing/main.py --log_directory=src/preprocessing/logs --input_directory=src/data/original/ --output_directory=src/data/processed/</p>"},{"location":"utils/data_utils/","title":"Data Utils","text":""},{"location":"utils/data_utils/#src.utils.data_utils.divide_markers","title":"<code>divide_markers(df)</code>","text":"<p>Divides the dataframe which contains all markers into a dictionary with  individual dataframes for each marker. </p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>the dataframe containing all markers</p> required <p>Returns:</p> Name Type Description <code>dfs_dict</code> <code>dict</code> <p>a dictionary with the marker names as keys and the</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def divide_markers(df: pandas.DataFrame):\n\"\"\"Divides the dataframe which contains all markers into a dictionary with \n    individual dataframes for each marker. \n\n    parameters:\n        df (pandas.DataFrame): the dataframe containing all markers\n\n    returns:\n        dfs_dict (dict): a dictionary with the marker names as keys and the\"\"\"\n    dfs_dict = {}\n\n    for marker in df.label.unique():\n        dfs_dict[marker] = (df.loc[(df['label']) == marker])\n\n    return dfs_dict\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.get_stats","title":"<code>get_stats(df, mapping)</code>","text":"<p>Calls the IQR method to get the stats of the data for each label</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input DataFrame with four columns, 'x', 'y', 'z', and 'label'.</p> required <code>mapping</code> <code>dict</code> <p>The mapping of the labels to the joints {key: label, value: number}</p> required <p>Returns:</p> Name Type Description <code>all_stats</code> <code>dict</code> <p>The stats of the data for each label.</p> <code>new_df</code> <code>pandas.DataFrame</code> <p>The df containing info of whether or not the timepoint is an outlier</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def get_stats(df: pandas.DataFrame , mapping: dict):\n\"\"\"Calls the IQR method to get the stats of the data for each label\n\n    Parameters:\n        df (pandas.DataFrame): The input DataFrame with four columns, 'x', 'y', 'z', and 'label'.\n        mapping (dict): The mapping of the labels to the joints {key: label, value: number}\n    Returns:\n            all_stats (dict): The stats of the data for each label.\n            new_df (pandas.DataFrame): The df containing info of whether or not the timepoint is an outlier \"\"\"\n\n    # order by index\n    df = df.sort_index()\n\n    # group by label\n    groups = df.groupby(\"label\")\n\n    new_dataframe = pd.DataFrame()\n    all_stats = {}\n\n    for name, group in groups:\n        # interpolate\n        group = group.drop(columns=[\"label\"])\n        group, stats = iqr_method(group, mapping[name], replace=False)\n\n        group[\"label\"] = name\n\n        all_stats[mapping[name]] = stats\n        new_dataframe = pd.concat([new_dataframe, group])\n\n    return all_stats, new_dataframe\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.interpolate","title":"<code>interpolate(df)</code>","text":"<p>interpolates the data to make it more uniform.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input DataFrame with four columns, 'x', 'y', 'z', and 'label'.</p> required <p>Returns:</p> Name Type Description <code>new_dataframe</code> <code>pandas.DataFrame</code> <p>The modified DataFrame with outliers replaced.</p> <code>new_df_outliers</code> <code>pandas.DataFrame</code> <p>The DataFrame with outliers marked as 1.</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def interpolate(df: pandas.DataFrame):\n\"\"\"interpolates the data to make it more uniform.\n\n    Parameters:\n        df (pandas.DataFrame): The input DataFrame with four columns, 'x', 'y', 'z', and 'label'.\n\n    Returns:\n        new_dataframe (pandas.DataFrame): The modified DataFrame with outliers replaced.\n        new_df_outliers (pandas.DataFrame): The DataFrame with outliers marked as 1.\"\"\"\n\n    # order by index\n    df = df.sort_index()\n\n    # group by label\n    groups = df.groupby(\"label\")\n\n    # interpolate\n    new_dataframe = pd.DataFrame()\n    new_df_outliers = pd.DataFrame()\n\n    for name, group in groups:\n        # interpolate\n        group = group.drop(columns=[\"label\"])\n        group, stats, df_outliers = iqr_method(group, name, replace=True)\n\n        group[\"label\"] = name\n        df_outliers[\"label\"] = name\n\n        new_dataframe = pd.concat([new_dataframe, group])\n        new_df_outliers = pd.concat([new_df_outliers, df_outliers])\n\n    # Removing rows with outlier values\n    outliers = ['outlier_x', 'outlier_y', 'outlier_z']\n    outlier_indices = new_df_outliers[new_df_outliers[outliers].any(axis=1)].index\n\n    new_df_outliers = new_df_outliers.drop(outlier_indices)\n    new_df_outliers = new_df_outliers.drop(['outlier_x', 'outlier_y', 'outlier_z'], axis=1)\n\n    return new_dataframe, new_df_outliers\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.iqr_method","title":"<code>iqr_method(group_data, label, replace=True)</code>","text":"<p>interpolates the data using the IQR method</p> <p>Parameters:</p> Name Type Description Default <code>group_data</code> <code>pandas.DataFrame</code> <p>The input DataFrame with three columns, 'x', 'y', 'z'</p> required <code>label</code> <code>str</code> <p>The label of the group aka the joint</p> required <code>replace</code> <code>bool</code> <p>If True, replaces the outliers with the previous or next value that is not an outlier (default: True)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas.DataFrame</code> <p>The modified DataFrame with outliers replaced. (if replace=True)</p> <code>stats</code> <code>dict</code> <p>The stats of the group_data</p> <code>group_data</code> <code>pandas.DataFrame</code> <p>The original DataFrame without outliers replaced. (if replace=False)</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def iqr_method(group_data: pandas.DataFrame, label: str, replace: bool=True):\n\"\"\"interpolates the data using the IQR method\n\n    Parameters:\n        group_data (pandas.DataFrame): The input DataFrame with three columns, 'x', 'y', 'z'\n        label (str): The label of the group aka the joint\n        replace (bool): If True, replaces the outliers with the previous or next value that is not an outlier (default: True)\n\n    Returns:\n        df (pandas.DataFrame): The modified DataFrame with outliers replaced. (if replace=True)\n        stats (dict): The stats of the group_data\n        group_data (pandas.DataFrame): The original DataFrame without outliers replaced. (if replace=False)\n    \"\"\"\n\n    # check if the group_data includes label\n    if \"label\" in group_data.columns:\n        # drop the label column\n        group_data = group_data.drop([\"label\"], axis=1)\n\n    Q1 = group_data.quantile(0.25)\n    Q3 = group_data.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # get the stats of each coordinate\n    x_stats = group_data[\"x\"].describe().to_dict()\n    y_stats = group_data[\"y\"].describe().to_dict()\n    z_stats = group_data[\"z\"].describe().to_dict()\n\n    # create a column and mark the outliers\n    group_data[\"outlier_x\"] = 0\n    group_data.loc[group_data[\"x\"] &lt; lower_bound[\"x\"], \"outlier_x\"] = 1\n    group_data.loc[group_data[\"x\"] &gt; upper_bound[\"x\"], \"outlier_x\"] = 1\n\n    group_data[\"outlier_y\"] = 0\n    group_data.loc[group_data[\"y\"] &lt; lower_bound[\"y\"], \"outlier_y\"] = 1\n    group_data.loc[group_data[\"y\"] &gt; upper_bound[\"y\"], \"outlier_y\"] = 1\n\n    group_data[\"outlier_z\"] = 0\n    group_data.loc[group_data[\"z\"] &lt; lower_bound[\"z\"], \"outlier_z\"] = 1\n    group_data.loc[group_data[\"z\"] &gt; upper_bound[\"z\"], \"outlier_z\"] = 1\n\n    # send the outliers to stat\n    outlier_x = group_data[group_data[\"outlier_x\"] == 1]\n    outlier_y = group_data[group_data[\"outlier_y\"] == 1]\n    outlier_z = group_data[group_data[\"outlier_z\"] == 1]\n\n    # save the stats to a dict\n    stats = {\n        \"percentages\": {\n            \"outlier_x\": len(outlier_x) / len(group_data),\n            \"outlier_y\": len(outlier_y) / len(group_data),\n            \"outlier_z\": len(outlier_z) / len(group_data),\n            \"label\": label,\n        },\n        \"x_stats\": x_stats,\n        \"y_stats\": y_stats,\n        \"z_stats\": z_stats,\n    }\n\n    df = group_data.copy()\n\n    if replace:\n        # interpolate the outliers\n        # Replace outliers in the x column\n        df = replace_outliers(df, \"x\")\n\n        # Replace outliers in the y column\n        df = replace_outliers(df, \"y\")\n\n        # Replace outliers in the z column\n        df = replace_outliers(df, \"z\")\n\n        # remove outlier column\n        df = df.drop([\"outlier_x\", \"outlier_y\", \"outlier_z\"], axis=1)\n\n        log.info(f\"Done interpolating outliers for label: {label}\")\n\n\n\n    return df, stats, group_data\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.open_original_to_df","title":"<code>open_original_to_df(file, path, to_numeric=False)</code>","text":"<p>Opens the original json file and converts it to a pandas dataframe</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The name of the file to open</p> required <code>to_numeric</code> <code>bool</code> <p>If True, converts the label to a number instead of a string (required for IQR method) (default: False)</p> <code>False</code> <code>path</code> <code>str</code> <p>The path to the file. If None, uses the input_directory flag</p> required <p>Returns:</p> Name Type Description <code>dataframe</code> <code>pandas.DataFrame</code> <p>The converted data with columns 'x', 'y', 'z', and 'label'</p> <code>mapping</code> <code>dict</code> <p>The mapping of the labels to the joints {key: label, value: number} (only if to_numeric is True)</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def open_original_to_df(file: str , path: str, to_numeric: bool=False):\n\"\"\"\n    Opens the original json file and converts it to a pandas dataframe\n\n    Parameters:\n        file (str): The name of the file to open\n        to_numeric (bool): If True, converts the label to a number instead of a string (required for IQR method) (default: False)\n        path (str): The path to the file. If None, uses the input_directory flag\n\n    Returns:\n        dataframe (pandas.DataFrame): The converted data with columns 'x', 'y', 'z', and 'label'\n        mapping (dict): The mapping of the labels to the joints {key: label, value: number} (only if to_numeric is True)\n\n    \"\"\"\n\n    if path is None:\n        path = FLAGS.input_directory\n\n    # read the json file\n    with open(os.path.join(path, file)) as json_file:\n        data = json.load(json_file)\n\n        # convert the data to a pandas dataframe\n        dataframe = pd.DataFrame()\n\n        log.info(\"Converting data to dataframe\")\n        for label in data[\"coords_3d\"].keys():\n            if label == \"com\":\n                continue\n            dataframe = to_dataframe(\n                dataframe, np.asanyarray(data[\"coords_3d\"][label][\"xyz\"]), label\n            )\n\n        # remove nontype values from dataframe\n        dataframe = dataframe[dataframe[\"x\"] != \"NoneType\"]\n        dataframe = dataframe[dataframe[\"y\"] != \"NoneType\"]\n        dataframe = dataframe[dataframe[\"z\"] != \"NoneType\"]\n\n        if to_numeric:\n            # convert the label to a number\n            mapping = {val: i for i, val in enumerate(dataframe[\"label\"].unique())}\n\n            # encode the column to an integer\n            dataframe[\"label\"] = dataframe[\"label\"].map(mapping)\n\n            # Convert the DataFrame to numeric dtype\n            df_numeric = dataframe.apply(pd.to_numeric, errors=\"coerce\")\n\n            # Check if all columns are numeric now\n            if df_numeric.select_dtypes(include=\"number\").columns.size == 0:\n                raise ValueError(\"DataFrame has no numeric columns\")\n\n            mapping = {v: k for k, v in mapping.items()}\n\n            return df_numeric, mapping\n\n\n        return dataframe, None\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.replace_outliers","title":"<code>replace_outliers(df, coordinate='z')</code>","text":"<p>Replaces outliers in a pandas DataFrame with the previous or next value that is not an outlier.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>The input DataFrame with two columns, 'value' and 'is_outlier'.</p> required <code>coordinate</code> <code>str</code> <p>The coordinate to replace the outliers in (default: 'z', options: 'x', 'y', 'z') </p> <code>'z'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas.DataFrame</code> <p>The modified DataFrame with outliers replaced.</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def replace_outliers(df: pandas.DataFrame, coordinate: str=\"z\"):\n\"\"\"\n    Replaces outliers in a pandas DataFrame with the previous or next value that is not an outlier.\n\n    Parameters:\n        df (pandas.DataFrame): The input DataFrame with two columns, 'value' and 'is_outlier'.\n        coordinate (str): The coordinate to replace the outliers in (default: 'z', options: 'x', 'y', 'z') \n\n    Returns:\n        df (pandas.DataFrame): The modified DataFrame with outliers replaced.\n    \"\"\"\n    # Create a new column called 'value2' that has the same values as 'value'\n    df[f\"{coordinate}_new\"] = df[f\"{coordinate}\"]\n\n    # if 'outlier_{coordinate}' == 1 replace with NaN\n    df.loc[df[f\"outlier_{coordinate}\"] == 1, f\"{coordinate}_new\"] = np.nan\n\n    # Forward fill NaNs with the next non-outlier value\n    df[f\"{coordinate}_new\"] = df[f\"{coordinate}_new\"].fillna(method=\"ffill\")\n\n    # Backward fill NaNs with the previous non-outlier value\n    df[f\"{coordinate}_new\"] = df[f\"{coordinate}_new\"].fillna(method=\"bfill\")\n\n    # Drop the f'{coordinate}' column and rename 'value2' to 'value'\n    df = df.drop(f\"{coordinate}\", axis=1).rename(\n        columns={f\"{coordinate}_new\": f\"{coordinate}\"}\n    )\n    return df\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.save_to_pickle","title":"<code>save_to_pickle(df, filename, path=None)</code>","text":"<p>Save the dataframe to pickle format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.Dataframe</code> <p>the dataframe to save</p> required <code>filename</code> <code>str</code> <p>the name of the file to save</p> required <code>path</code> <code>str</code> <p>The path to the file. If None, uses the output_directory flag</p> <code>None</code> side effects <p>Saves the dataframe to the output directory as a pickle file</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def save_to_pickle(df : pandas.DataFrame, filename: str , path: str=None):\n\"\"\" \n    Save the dataframe to pickle format.\n\n    parameters:\n        df (pandas.Dataframe): the dataframe to save\n        filename (str): the name of the file to save\n        path (str): The path to the file. If None, uses the output_directory flag\n\n    side effects:\n        Saves the dataframe to the output directory as a pickle file\n    \"\"\"\n    if path is None:\n        path = flags.FLAGS.output_directory\n\n    df = df.rename_axis(\"MyIdx\").sort_values(\n        by=[\"MyIdx\", \"label\"], ascending=[True, True]\n    )\n\n    log.info(f\"Saving pickle to {path}\")\n\n    if not os.path.exists(path):\n        log.info(f\"Creating folder: {path}\")\n        os.makedirs(path)\n\n    df.to_pickle(f\"{path}/{filename}.pkl\")\n\n    return\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.save_to_tensor","title":"<code>save_to_tensor(df, filename, type, path=None, save_labels=True)</code>","text":"<p>Converts the dataframe to a tensor and saves it to the output directory</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>the dataframe to save</p> required <code>filename</code> <code>str</code> <p>the name of the file to save</p> required <code>type</code> <code>str</code> <p>the type of tensor to save (hip_centered or coordinates)</p> required <code>path</code> <code>str</code> <p>The path to the file. If None, uses the output_directory flag</p> <code>None</code> <code>save_labels</code> <code>bool</code> <p>If true, saves the labels as a pickle file</p> <code>True</code> side effects <p>saves the file to the folder as a tensor file with the as type/filename.pt</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def save_to_tensor(df :pd.DataFrame , filename: str, type: str, path: str=None, save_labels: bool=True):\n\"\"\"\n    Converts the dataframe to a tensor and saves it to the output directory\n\n\n    parameters:\n        df (pandas.DataFrame):  the dataframe to save\n        filename (str): the name of the file to save\n        type (str): the type of tensor to save (hip_centered or coordinates)\n        path (str): The path to the file. If None, uses the output_directory flag\n        save_labels (bool): If true, saves the labels as a pickle file\n\n    side effects:\n        saves the file to the folder as a tensor file with the as type/filename.pt\n\n    \"\"\"\n\n\n    df = df.rename_axis(\"MyIdx\").sort_values(\n        by=[\"MyIdx\", \"label\"], ascending=[True, True]\n    )\n\n    if path is None:\n        path = flags.FLAGS.output_directory\n\n\n    if save_labels:\n        unique_labels = df['label'].unique()   \n        unique_labels_dict = {i:label for i, label in enumerate(unique_labels)}\n\n\n        folder_labels = os.path.join(flags.FLAGS.output_directory, f\"labels.pkl\")\n\n        if not os.path.exists(folder_labels):\n            # Save the dictionary as a pickle file\n            with open(folder_labels, 'wb') as f:\n                pickle.dump(unique_labels_dict, f)\n\n    # drop the label values\n    df_tmp = df.drop(columns=[\"label\"])\n\n    # combine the columns into list\n    df_tmp[\"combined\"] = df_tmp.values.tolist()\n    # join the rows with the same index\n    df_tmp = df_tmp.groupby(\"MyIdx\")[\"combined\"].apply(list).reset_index()\n\n    # # convert the list of coordinates into a torch tensor\n    model_tensor = nn.tensor(df_tmp[\"combined\"].values.tolist())\n\n    folder = type\n\n    # reshape the tensor so that the shape is (number of frames, ??? ) \n    model_tensor = model_tensor.view(\n        model_tensor.shape[0], model_tensor.shape[1] * model_tensor.shape[2])\n\n    # save the tensor\n    folder = os.path.join(flags.FLAGS.output_directory, folder)\n\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    log.info(f\"Saving tensor to {folder}/{filename}.pt\")\n\n    nn.save(model_tensor, f\"{folder}/{filename}.pt\")\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.select_n_random_files","title":"<code>select_n_random_files(file_list, n=1)</code>","text":"<p>Selects n random files from the list of files</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list[str]</code> <p>The list of files to select from </p> required <code>n</code> <code>int</code> <p>The number of random files to select (default: 1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>selected_files</code> <code>list[str]</code> <p>A list of paths of the randomly selected files</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def select_n_random_files(file_list , n: int = 1):\n\"\"\"\n    Selects n random files from the list of files\n\n    parameters:\n        file_list (list[str]): The list of files to select from \n        n (int): The number of random files to select (default: 1)\n\n    returns:\n        selected_files (list[str]): A list of paths of the randomly selected files\n    \"\"\"\n\n    selected_files = random.sample(file_list, n)\n\n    return selected_files\n</code></pre>"},{"location":"utils/data_utils/#src.utils.data_utils.to_dataframe","title":"<code>to_dataframe(dataframe, data, label)</code>","text":"<p>converts csv data to pandas dataframe, separating the x y z and labels</p> Source code in <code>src/utils/data_utils.py</code> <pre><code>def to_dataframe(dataframe, data, label):\n\"\"\"converts csv data to pandas dataframe, separating the x y z and labels\"\"\"\n\n    d = {'x': data[0], 'y': data[1], 'z':data[2], 'label': label}\n    df = pd.DataFrame(data=d)\n    dataframe = pd.concat([df, dataframe])\n    return dataframe\n</code></pre>"},{"location":"utils/file_utils/","title":"File Utils","text":""},{"location":"utils/file_utils/#src.utils.file_utils.open_n_original_files","title":"<code>open_n_original_files(input_dir, n)</code>","text":"<p>Opens n random files from the original dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>path to the original dataset</p> required <code>n</code> <code>int</code> <p>number of files to open</p> required <p>Returns:</p> Name Type Description <code>files</code> <code>list</code> <p>list of n random files</p> Source code in <code>src/utils/file_utils.py</code> <pre><code>def open_n_original_files(input_dir, n):\n\"\"\"\n    Opens n random files from the original dataset.\n\n    Args:\n        input_dir (str): path to the original dataset\n        n (int): number of files to open\n\n    Returns:\n        files (list): list of n random files\n\n\n    \"\"\"\n\n    setup_utils.safety_check(input_dir, exist=True, is_dir=True)\n\n    # open the folder and randomly select n files\n    files = [file for file in os.listdir(input_dir) if file.endswith(\".json\")]\n\n    # if the number of files is less than n, set n to the number of files\n    if len(files) &lt; n:\n        log.warning(\n            \"Number of files is less than n, setting n to the number of files\")\n        n = len(files)\n\n    files = nn.utils.data.random_split(files, [n, len(files) - n])[0]\n    files = [file for file in files if file.endswith(\".json\")]\n\n    return files\n</code></pre>"},{"location":"utils/plot_utils/","title":"Plot Utils","text":"<p>This script is used to visualize the data from the json files in order to explore the data</p>"},{"location":"utils/plot_utils/#src.utils.plot_utils.monkey_video","title":"<code>monkey_video()</code>","text":"<p>Loads pickled DataFrames containing processed video data and creates a video by joining the frames.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the original data directory or its pickle subdirectory does not exist.</p> Side effects <ul> <li>Loads n random pickled DataFrames from the pickle subdirectory of the original data directory specified in the flags.</li> <li>Calls the <code>plot_video()</code> function from <code>monkey_video</code> module to join the frames and create the output video.</li> <li>Logs progress and errors.</li> </ul> Source code in <code>src/utils/plot_utils.py</code> <pre><code>def monkey_video():\n\"\"\"\n    Loads pickled DataFrames containing processed video data and creates a video by joining the frames.\n\n    Raises:\n        FileNotFoundError: If the original data directory or its pickle subdirectory does not exist.\n\n    Side effects:\n        - Loads n random pickled DataFrames from the pickle subdirectory of the original data directory specified in the flags.\n        - Calls the `plot_video()` function from `monkey_video` module to join the frames and create the output video.\n        - Logs progress and errors.\n\n    \"\"\"\n\n    log.info(\"Plotting monkey video\")\n\n    # 1. The data is already processed and saved in the data folder (by the stats.py script)\n    # 2. We need to load n random data\n    # 3. create a video with the frames\n\n    dataframe_path = os.path.join(flags.original_data_directory, \"pickle\")\n\n    # check if the folder is empty\n    try:\n        if len(os.listdir(dataframe_path)) == 0:\n            log.error(\n                f\"The directory {dataframe_path} does not exist. Run the stats.py script first\"\n            )\n    except FileNotFoundError:\n        raise Exception(f\"Directory {dataframe_path} does not exist\")\n\n    if len(os.listdir(dataframe_path)) == 0:\n        log.error(\n            \"{dataframe_path} Folder is empty, run the stats.py script first\")\n        exit(0)\n\n    # select n random files\n\n    n = flags.FLAGS.video_files\n\n    # open the folder and get the list of files\n\n    file_list = os.listdir(dataframe_path)\n\n    if len(file_list) &lt; n:\n        log.warning(\n            f\"Number of files is less than {n}, using {len(file_list)} files\")\n        n = len(file_list)\n\n    files = du.select_n_random_files(file_list, n)\n\n    # Open the pickle file\n    for file in tqdm(files, desc='Processing files'):\n        print(\"Opening file: {}\".format(os.path.join(dataframe_path, file)))\n        my_df = pd.read_pickle(os.path.join(dataframe_path, file))\n\n        # remove the pkl and json extension from the file name\n        file = file.replace(\".pkl\", \"\").replace(\".json\", \"\")\n\n        mkv.plot_video(my_df, flags.FLAGS.time, file)\n\n    log.info(f\"Loading data for monkey video from {dataframe_path}\")\n    print(f\"Loading data for monkey video from {dataframe_path}\")\n\n    print(files)\n</code></pre>"},{"location":"utils/plot_utils/#src.utils.plot_utils.outlier_event_plot","title":"<code>outlier_event_plot(stats_dir=None, output_file=None)</code>","text":"<p>Loads pickled DataFrames containing outlier statistics and plots the events on separate heatmaps for each joint coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>stats_dir</code> <code>str</code> <p>The path to the directory containing the pickled DataFrames. If None, the path specified in the flags is used.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>The path to the output file. If None, the path specified in the flags is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the stats directory or its DataFrame subdirectory does not exist.</p> Side effects <ul> <li>Loads all pickled DataFrames from the DataFrame subdirectory of the stats directory specified in the flags.</li> <li>Concatenates the DataFrames vertically.</li> <li>Plots the outliers events on separate heatmaps for each joint coordinate.</li> <li>Saves the heatmaps as PNG files in the output directory specified in the flags.</li> <li>Logs progress and errors.</li> </ul> Source code in <code>src/utils/plot_utils.py</code> <pre><code>def outlier_event_plot(stats_dir : str = None, output_file : str = None):\n\"\"\"\n    Loads pickled DataFrames containing outlier statistics and plots the events on separate heatmaps for each joint coordinate.\n\n    Parameters: \n        stats_dir (str): The path to the directory containing the pickled DataFrames. If None, the path specified in the flags is used.\n        output_file (str): The path to the output file. If None, the path specified in the flags is used.\n\n\n    Raises:\n        FileNotFoundError: If the stats directory or its DataFrame subdirectory does not exist.\n\n    Side effects:\n        - Loads all pickled DataFrames from the DataFrame subdirectory of the stats directory specified in the flags.\n        - Concatenates the DataFrames vertically.\n        - Plots the outliers events on separate heatmaps for each joint coordinate.\n        - Saves the heatmaps as PNG files in the output directory specified in the flags.\n        - Logs progress and errors.\n\n    \"\"\"\n\n    # open the stats folder\n    log.info(\"Plotting outliers in historgram\")\n    print('Ploting Outlier Events')\n\n    if stats_dir is None:\n        stats_dir = flags.stats_directory\n\n    stats_df_dir = os.path.join(stats_dir, 'stats_df')\n\n    # check if the stats folder exists\n    if not os.path.exists(stats_df_dir):\n        log.error(\n            f\"The stats directory {stats_df_dir} does not exist. Run the stats.py script first\"\n        )\n        exit()\n\n    log.info(f\"Stats directory: {stats_df_dir}\")\n\n    concatenated_df = pd.DataFrame()\n\n    # load all the json files in the folder\n    for file in tqdm(os.listdir(stats_df_dir), desc='Processing files'):\n        if file.endswith(\".pkl\"):\n            path = os.path.join(stats_df_dir, file)\n            # load the pandas dataframe\n            data = pd.read_pickle(path)\n\n            # drop the 'x' y and 'z' columns\n            data = data.drop(['x', 'y', 'z'], axis=1)\n\n            # Concatenate the dataframes vertically\n            concatenated_df = pd.concat([concatenated_df, data])\n\n    # get a list of unique joints in the DataFrame\n\n    # get a list of unique joints in the DataFrame\n    joints = concatenated_df['label'].unique()\n\n    # save\n    if output_file is None:\n        output_file = flags.FLAGS.output_directory\n\n    output_file = os.path.join(output_file, \"histogram\")\n\n    # create the folder if it doesn't exist\n    if not os.path.exists(output_file):\n        os.makedirs(output_file)\n\n    # iterate through each joint with tqdm\n    for i, joint in enumerate(tqdm(joints, desc='Creating XYZ heatmaps')):\n\n        fig, ax = plt.subplots(figsize=(5*len(joints), 5))\n\n        # subset the DataFrame by joint\n        subset = concatenated_df[concatenated_df['label'] == joint]\n\n        # remove the label\n        subset = subset.drop('label', axis=1)\n\n        # get the indexes that are outliers\n        x_np = subset['outlier_x'].to_numpy()\n        y_np = subset['outlier_y'].to_numpy()\n        z_np = subset['outlier_z'].to_numpy()\n\n        spike_times_x = [i for i, x in enumerate(x_np) if x == 1]\n        spike_times_y = [i for i, x in enumerate(y_np) if x == 1]\n        spike_times_z = [i for i, x in enumerate(z_np) if x == 1]\n\n        # plot the lines\n        plt.vlines(spike_times_x, 0, 0.5, color='r')\n        plt.vlines(spike_times_y, 0, 0.5, color='b')\n        plt.vlines(spike_times_z, 0, 0.5, color='g')\n\n        # set the title of the current subplot\n        plt.set_title(f'{joint}')\n\n        plt.set_ylabel('XYX')\n\n        plt.set_xlim([0, len(x_np)])\n        plt.set_xlabel('Time 30fps')\n\n        fig.tight_layout()\n\n        fig.savefig(f'{output_file}/{joint}_XYZ.png')\n</code></pre>"},{"location":"utils/plot_utils/#src.utils.plot_utils.outliers_boxplot","title":"<code>outliers_boxplot(json_path=None, n_files=None, output_file=None)</code>","text":"<p>Loads a specified number of original JSON files and plots a boxplot for each joint coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>n_files</code> <code>int</code> <p>The number of files to open.</p> <code>None</code> <code>json_path</code> <code>str</code> <p>The path to the directory containing the original JSON files. If None, the path specified in the flags is used.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>The path to the output file. If None, the path specified in the flags is used.</p> <code>None</code> Side effects <ul> <li>Opens and concatenates a specified number of randomly selected original JSON files.</li> <li>Converts the x, y, and z columns of the resulting DataFrame to float type.</li> <li>Groups the DataFrame by joint coordinate and plots a boxplot for each group.</li> <li>Saves the boxplot as a PNG file in the output directory specified in the flags.</li> <li>Logs progress and errors.</li> </ul> Source code in <code>src/utils/plot_utils.py</code> <pre><code>def outliers_boxplot(json_path: str = None, n_files: int = None, output_file: str = None):\n\"\"\"\n    Loads a specified number of original JSON files and plots a boxplot for each joint coordinate.\n\n    Parameters: \n        n_files (int): The number of files to open.\n        json_path (str): The path to the directory containing the original JSON files. If None, the path specified in the flags is used.\n        output_file (str): The path to the output file. If None, the path specified in the flags is used.\n\n    Side effects:\n        - Opens and concatenates a specified number of randomly selected original JSON files.\n        - Converts the x, y, and z columns of the resulting DataFrame to float type.\n        - Groups the DataFrame by joint coordinate and plots a boxplot for each group.\n        - Saves the boxplot as a PNG file in the output directory specified in the flags.\n        - Logs progress and errors.\n\n    \"\"\"\n\n    # open the original json files and randomly select n files\n    log.info(\"Plotting outliers boxplot\")\n\n    if json_path is None:\n        json_path = os.path.join(flags.FLAGS.original_data_directory, \"json\")\n\n    if n_files is None:\n        n_files = flags.FLAGS.n_files\n\n    files = lu.open_n_original_files(json_path, n_files)\n\n    all_files_df = pd.DataFrame()\n    for file in files:\n        dataframe, _ = du.open_original_to_df(file, False, json_path)\n\n        all_files_df = pd.concat([all_files_df, dataframe])\n\n    log.info(\"Finished opening files\")\n\n    # convert the columns xyz to float type\n    all_files_df[\"x\"] = all_files_df[\"x\"].astype(float)\n    all_files_df[\"y\"] = all_files_df[\"y\"].astype(float)\n    all_files_df[\"z\"] = all_files_df[\"z\"].astype(float)\n\n    # group the dataframe by the joint and drop the label column\n    groups = all_files_df.groupby(\"label\", group_keys=True).apply(\n        lambda x: x.drop(\"label\", axis=1)\n    )\n\n    log.info(\"Plotting boxplot\")\n\n    # for each group plot the boxplot\n    groups.boxplot(rot=45, fontsize=12, figsize=(8, 10))\n\n    # set the x axis label\n    plt.xlabel(\"Coordinate\")\n\n    # set the y axis label\n    plt.ylabel(\"Value\")\n\n    if output_file is None:\n        output_file = os.path.join(flags.FLAGS.output_directory, \"boxplot\")\n\n\n    # create the folder if it doesn't exist\n    if not os.path.exists(output_file):\n        os.makedirs(output_file)\n\n    output_file = f\"{output_file}/boxplot.png\"\n\n    log.info(f\"Saving boxplot to: {output_file}\")\n\n    plt.savefig(output_file)\n</code></pre>"},{"location":"utils/plot_utils/#src.utils.plot_utils.outliers_percent","title":"<code>outliers_percent(stats_dir)</code>","text":"<p>Loads JSON files containing outlier statistics and plots the average percentage of outliers for each joint and the overall mean.</p> <p>Parameters:</p> Name Type Description Default <code>stats_dir</code> <code>str</code> <p>The path to the directory containing the JSON files with the outlier statistics. If None, the path specified in the flags is used.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the stats directory or its overview subdirectory does not exist.</p> Side effects <ul> <li>Saves individual joint outliers plots and an overall mean outliers plot as PNG files in the output directory specified in the flags.</li> <li>Logs progress and errors.</li> </ul> Source code in <code>src/utils/plot_utils.py</code> <pre><code>def outliers_percent(stats_dir: str):\n\"\"\"\n    Loads JSON files containing outlier statistics and plots the average percentage of outliers for each joint and the overall mean.\n\n    Parameters:\n        stats_dir (str): The path to the directory containing the JSON files with the outlier statistics. If None, the path specified in the flags is used.\n\n    Raises:\n        FileNotFoundError: If the stats directory or its overview subdirectory does not exist.\n\n    Side effects:\n        - Saves individual joint outliers plots and an overall mean outliers plot as PNG files in the output directory specified in the flags.\n        - Logs progress and errors.\n\n    \"\"\"\n\n    log.info(\"Plotting outliers\")\n\n    stats_overview_dir = os.path.join(stats_dir, 'stats_overview')\n\n    # check if the stats folder exists\n    if not os.path.exists(stats_overview_dir):\n        log.error(\n            f\"The stats directory {stats_overview_dir} does not exist. Run the stats.py script first\"\n        )\n        exit()\n\n    log.info(\"Stats directory: {}\".format(stats_overview_dir))\n\n    data = False\n    concatenated_df = pd.DataFrame()\n    # load all the json files in the folder\n    for file in os.listdir(stats_overview_dir):\n        if file.endswith(\".json\"):\n            path = os.path.join(stats_overview_dir, file)\n            # load the json file\n            with open(path, \"r\") as f:\n                data = json.load(f)\n                log.info(\"Json file loaded\")\n\n            data = {key: value[\"percentages\"] for key, value in data.items()}\n\n            data = json_to_pandas(data)\n\n            # Concatenate the dataframes vertically\n            concatenated_df = pd.concat([concatenated_df, data])\n\n    # copy the index into a new column\n    concatenated_df[\"coordinate\"] = concatenated_df.index\n\n    # reset the index\n    concatenated_df.reset_index(inplace=True, drop=True)\n\n    grouped_df = concatenated_df.groupby(\"coordinate\").mean()\n\n    # multiple all the values by 100\n    grouped_df = grouped_df * 100\n\n    # for each joint plot the average of the outliers\n    for joint in grouped_df.columns:\n        # plot the average of the outliers for each joint\n        outliers_plot(grouped_df[joint], joint)\n\n    # Calculate the row-wise mean\n    grouped_df[\"Mean\"] = grouped_df.mean(axis=1)\n\n    # plot the average of the outliers for each joint\n    outliers_plot(grouped_df[\"Mean\"], \"Mean\")\n\n    log.info(\"Outliers plotted\")\n</code></pre>"},{"location":"utils/plot_utils/#src.utils.plot_utils.outliers_plot","title":"<code>outliers_plot(df, title)</code>","text":"<p>Plot the average percentage of outliers for each joint in a bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>A DataFrame containing the average percentage of outliers for each joint.</p> required <code>title</code> <code>str</code> <p>The title of the plot.</p> required Side effects <ul> <li>Saves the plot as a PNG file in the output directory specified in the flags.</li> </ul> Source code in <code>src/utils/plot_utils.py</code> <pre><code>def outliers_plot(df : pandas.DataFrame, title:  str):\n\"\"\"\n    Plot the average percentage of outliers for each joint in a bar chart.\n\n    Parameters:\n        df (pandas.DataFrame): A DataFrame containing the average percentage of outliers for each joint.\n        title (str): The title of the plot.\n\n    Side effects:\n        - Saves the plot as a PNG file in the output directory specified in the flags.\n    \"\"\"\n    # plot the average of the outliers for each joint\n\n    df.plot(kind=\"bar\", figsize=(10, 8))\n\n    # set plot max y value to 100\n    plt.ylim(0, 100)\n\n    # set the x axis label\n    plt.xlabel(\"Coordinate\")\n\n    # set the y axis label\n    plt.ylabel(\"Percentage of outliers\")\n\n    plt.title(title)\n\n    # add labels of the percentage of outliers\n    for index, value in enumerate(df):\n        plt.text(index, value, str(round(value, 2)) + \"%\")\n\n    output_file = os.path.join(flags.FLAGS.output_directory, \"outliers\")\n    output_file = f\"{output_file}/{title}.png\"\n\n    # save the plot\n    plt.savefig(output_file)\n\n    # close the plot\n    plt.close()\n</code></pre>"},{"location":"utils/setup_utils/","title":"Setup Utils","text":""},{"location":"utils/setup_utils/#src.utils.setup_utils.create_folder","title":"<code>create_folder(folder_path)</code>","text":"<p>Create the folder for the project if the folder does not exist it will create it</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The path of the folder to check</p> required <p>:return: None</p> Source code in <code>src/utils/setup_utils.py</code> <pre><code>def create_folder(folder_path):\n\"\"\"\n    Create the folder for the project if the folder does not exist it will create it\n\n    Params:\n        folder_path (str): The path of the folder to check\n\n    :return: None\n    \"\"\"\n    # check if the folder exist for each sub-folder\n    # divide the path in sub-folders\n    sub_folders = folder_path.split(os.sep)\n\n    current = sub_folders[0]\n\n    for folder in sub_folders[1:]:\n        # check if the folder exists if not create it\n        if not os.path.exists(current):\n            os.mkdir(current)\n            log.info(f\"Created the folder {current}\")\n        current = os.path.join(current, folder)\n\n    # check if the folder exists if not create it\n    if not os.path.exists(current):\n        os.mkdir(current)\n        log.info(f\"Created the folder {current}\")\n</code></pre>"},{"location":"utils/setup_utils/#src.utils.setup_utils.logger_setup","title":"<code>logger_setup(log_dir=None, log_level=None)</code>","text":"<p>Setup the logger for the project and creates a log file in the log directory.</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>The path of the folder to check if None it will use the log_directory from the flags</p> <code>None</code> <code>log_level</code> <code>str</code> <p>The log level to use if None it will use the log_level from the flags</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>logging</code> <p>The logger object</p> Source code in <code>src/utils/setup_utils.py</code> <pre><code>def logger_setup(log_dir : str = None , log_level : str =None ):\n\"\"\"\n    Setup the logger for the project and creates a log file in the log directory.\n\n    Params:\n        log_dir (str): The path of the folder to check if None it will use the log_directory from the flags\n        log_level (str): The log level to use if None it will use the log_level from the flags\n\n    Returns:\n        logger (logging): The logger object\n\n    \"\"\"\n\n    ##############################################\n    # Create logger directory\n    ##############################################\n\n    if log_dir is None:\n        log_dir = flags.FLAGS.log_directory\n\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # If log_dir is not empty, create a new enumerated sub-directory in it for\n    # logger.\n    list_log_dir = os.listdir(log_dir)\n\n    if len(list_log_dir) != 0:  # For safety, explicitly use len instead of bool\n        existing_log_subdirs = [\n            int(filename) for filename in list_log_dir if filename.isdigit()\n        ]\n        if not existing_log_subdirs:\n            existing_log_subdirs = [-1]\n        new_log_subdir = str(max(existing_log_subdirs) + 1)\n        log_dir = os.path.join(log_dir, new_log_subdir)\n        os.mkdir(log_dir)\n    else:\n        log_dir = os.path.join(log_dir, \"0\")\n        os.mkdir(log_dir)\n\n    ##############################################\n    # Load config\n    ##############################################\n\n    LOG_FILENAME = r\"log_file.out\"\n    # join the log directory with the log file name\n\n    LOG_FILENAME = os.path.join(log_dir, LOG_FILENAME)\n\n    logger = logging\n\n    if log_level is None:\n        log_level = flags.FLAGS.log_level\n\n    logging.basicConfig(\n        filename=LOG_FILENAME,\n        format=\"%(asctime)s %(levelname)s %(filename)s:%(lineno)d - %(message)s\",\n        level=log_level,\n        force=True,\n    )\n\n    # logger.basicConfig(filename=LOG_FILENAME, format='%(asctime)s - %(message)s', level=flags.FLAGS.log_level)\n\n    logger.info(\"Log directory: {}\".format(log_dir))\n    return logger\n</code></pre>"},{"location":"utils/setup_utils/#src.utils.setup_utils.safety_check","title":"<code>safety_check(folder_path, exist=True, is_dir=True)</code>","text":"<p>Perform a safety check on the input and output folder</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The path of the folder to check </p> required <code>exist</code> <code>bool</code> <p>If True, check if the folder exists (default: True)</p> <code>True</code> <code>is_dir</code> <code>bool</code> <p>If True, check if the folder is a directory, else check if it is a file (default: True)</p> <code>True</code> Side effects <p>If the folder does not exist it will log the error and exit the program</p> Source code in <code>src/utils/setup_utils.py</code> <pre><code>def safety_check(folder_path : str , exist=True, is_dir=True):\n\"\"\"\n    Perform a safety check on the input and output folder\n\n    Params:\n        folder_path (str): The path of the folder to check \n        exist (bool): If True, check if the folder exists (default: True)\n        is_dir (bool): If True, check if the folder is a directory, else check if it is a file (default: True)\n\n    Side effects:\n        If the folder does not exist it will log the error and exit the program\n\n    \"\"\"\n\n    log.info(f\"Starting safety check on {folder_path}\")\n\n    if exist:\n        # is it a folder?\n        if os.path.isdir(folder_path):\n            try:\n                # check if the folder exists\n                if not os.path.exists(folder_path):\n                    raise FileNotFoundError(f\"The input folder {folder_path} does not exist\")\n            except FileNotFoundError as e:\n                log.error(e)\n                print(e)\n                exit(1)\n        else:\n            try:\n                # check if the file exists\n                if not os.path.exists(folder_path):\n                    raise FileNotFoundError(f\"The file {folder_path} does not exist\")\n            except FileNotFoundError as e:\n                log.error(e)\n                print(e)\n                exit(1)\n\n    if is_dir:\n        # check if the input file is a folder\n        try:\n            if not os.path.isdir(folder_path):\n                raise NotADirectoryError(f\"The {folder_path} is not a folder\")\n        except NotADirectoryError as e:\n            log.error(e)\n            print(e)\n            sys.exit(1)\n    else:\n        # check if the input file is a file\n        try:\n            if not os.path.isfile(folder_path):\n                raise FileNotFoundError(f\"The {folder_path} is not a file\")\n        except FileNotFoundError as e:\n            log.error(e)\n            print(e)\n            sys.exit(1)\n</code></pre>"},{"location":"utils/setup_utils/#src.utils.setup_utils.setup_subfolders","title":"<code>setup_subfolders(folder_path, sub_folders)</code>","text":"<p>Setup the folders for the project if the sub-folders do not exist it will create them</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The path of the folder to check</p> required <code>sub_folders</code> <code>list</code> <p>The list of sub-folders to check</p> required <p>:return: None</p> Source code in <code>src/utils/setup_utils.py</code> <pre><code>def setup_subfolders(folder_path, sub_folders):\n\"\"\"\n    Setup the folders for the project if the sub-folders do not exist it will create them\n\n    Params:\n        folder_path (str): The path of the folder to check\n        sub_folders (list): The list of sub-folders to check\n\n    :return: None\n    \"\"\"\n\n    # check if the folder exists\n    if not os.path.exists(folder_path):\n        # create the folder\n        os.mkdir(folder_path)\n        log.info(f\"Created the folder {folder_path}\")\n\n    # check if the sub-folders exists\n    for sub_folder in sub_folders:\n        # check if the sub-folder exists\n        if not os.path.exists(os.path.join(folder_path, sub_folder)):\n            # create the sub-folder\n            os.mkdir(os.path.join(folder_path, sub_folder))\n            log.info(f\"Created the folder {sub_folder}\")\n\n        else:\n            log.warn(\n                f\"The folder {sub_folder} already exists, the files will be overwritten\"\n            )\n</code></pre>"},{"location":"utils/setup_utils/#src.utils.setup_utils.slash_check","title":"<code>slash_check(path)</code>","text":"<p>Checks that the path does not end with a slash, if not it adds it</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to check</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>The path with a slash at the end</p> Source code in <code>src/utils/setup_utils.py</code> <pre><code>def slash_check(path: str):\n\"\"\"\n    Checks that the path does not end with a slash, if not it adds it\n\n    Args:\n        path (str): The path to check\n\n    Returns:\n        path (str): The path with a slash at the end\n    \"\"\"\n\n    log.info(f\"Checking if the path {path} ends with a slash\")\n\n    if path[-1] != \"/\":\n        path += \"/\"\n        log.info(f\"Path {path} does not end with a slash, adding it\")\n        return path\n    else:\n        log.info(f\"Path {path} ends with a slash\")\n        return path\n</code></pre>"}]}